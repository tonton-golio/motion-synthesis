{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataModule params:\n",
      "\tbatch_size: 256\n",
      "\tpath: /Users/tonton/Documents/motion-synthesis/mnist_latent_diffusion/\n",
      "\trotation: 0\n",
      "\tscale: 0\n",
      "\ttranslate: (0, 0)\n",
      "\tshear: 0\n",
      "\tnormalize: (0.1307, 0.3081)\n",
      "\tbool: False\n",
      "len before 60000 10000\n",
      "len before 60000 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type           | Params\n",
      "--------------------------------------------\n",
      "0 | model    | ImageDiffusion | 1.3 M \n",
      "1 | criteria | MSELoss        | 0     \n",
      "--------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.322     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc44e66175e48faab9f48204db8982c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea6a106ad5e48feaa0817003c8539ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fd9f8dfbb849e59eed711139977017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68de63b9ce9e456d8d79b33a4e9e589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81e9eb9a5fd4c0dbe0e8c11dc583282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5741328d037648e6b3b2f11909d2d35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f4aa4092364a9993df96e9d82a23ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6120dd6a4cff4c6a9bbcb82ed9da82ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cec52779474fef8d7114051efa1007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2c1bc786b24b398ea154f42c931b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc69c8c8ba854097a901a2ea53571979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62524c722f6e4b4dbf4d799110d00d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d01707d6f948bd8860891e8f97c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cfe57d4da64f14b65fa38d65072bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73ea8d5f1a0404bb37aaf8398c5e71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b03d0d84e04bbb99f6a2a1dced1778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47488618a9c457190baa26d8d3b8e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e993867819db4ce9a7cdb147c8e791f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dc4faf629f4c5598ac6338ec024726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7a8ec165e41e7bb6443065c197554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d712c300d94d659ac70a9efe036021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de8a178adb74ac59415f128d0ffc125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de24ae67854418da3bfded5e17919df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc094b516f443289b32cf40c477712e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10247b6c44d2471faed7d7f1ce9364b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09fc078a8b54f64beedccb7a9920445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2217f243de4ce881dea9f5e01d279f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonton/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from VAE.dataset import MNISTDataModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self,groups):\n",
    "        super().__init__()\n",
    "        self.groups=groups\n",
    "    def forward(self,x):\n",
    "        n,c,h,w=x.shape\n",
    "        x=x.view(n,self.groups,c//self.groups,h,w) # group\n",
    "        x=x.transpose(1,2).contiguous().view(n,-1,h,w) #shuffle\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ConvBnSiLu(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
    "        super().__init__()\n",
    "        self.module=nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding),\n",
    "                                  nn.BatchNorm2d(out_channels),\n",
    "                                  nn.SiLU(inplace=True))\n",
    "    def forward(self,x):\n",
    "        return self.module(x)\n",
    "\n",
    "class ResidualBottleneck(nn.Module):\n",
    "    '''\n",
    "    shufflenet_v2 basic unit(https://arxiv.org/pdf/1807.11164.pdf)\n",
    "    '''\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1=nn.Sequential(nn.Conv2d(in_channels//2,in_channels//2,3,1,1,groups=in_channels//2),\n",
    "                                    nn.BatchNorm2d(in_channels//2),\n",
    "                                    ConvBnSiLu(in_channels//2,out_channels//2,1,1,0))\n",
    "        self.branch2=nn.Sequential(ConvBnSiLu(in_channels//2,in_channels//2,1,1,0),\n",
    "                                    nn.Conv2d(in_channels//2,in_channels//2,3,1,1,groups=in_channels//2),\n",
    "                                    nn.BatchNorm2d(in_channels//2),\n",
    "                                    ConvBnSiLu(in_channels//2,out_channels//2,1,1,0))\n",
    "        self.channel_shuffle=ChannelShuffle(2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1,x2=x.chunk(2,dim=1)\n",
    "        x=torch.cat([self.branch1(x1),self.branch2(x2)],dim=1)\n",
    "        x=self.channel_shuffle(x) #shuffle two branches\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResidualDownsample(nn.Module):\n",
    "    '''\n",
    "    shufflenet_v2 unit for spatial down sampling(https://arxiv.org/pdf/1807.11164.pdf)\n",
    "    '''\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1=nn.Sequential(nn.Conv2d(in_channels,in_channels,3,2,1,groups=in_channels),\n",
    "                                    nn.BatchNorm2d(in_channels),\n",
    "                                    ConvBnSiLu(in_channels,out_channels//2,1,1,0))\n",
    "        self.branch2=nn.Sequential(ConvBnSiLu(in_channels,out_channels//2,1,1,0),\n",
    "                                    nn.Conv2d(out_channels//2,out_channels//2,3,2,1,groups=out_channels//2),\n",
    "                                    nn.BatchNorm2d(out_channels//2),\n",
    "                                    ConvBnSiLu(out_channels//2,out_channels//2,1,1,0))\n",
    "        self.channel_shuffle=ChannelShuffle(2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=torch.cat([self.branch1(x),self.branch2(x)],dim=1)\n",
    "        x=self.channel_shuffle(x) #shuffle two branches\n",
    "\n",
    "        return x\n",
    "\n",
    "class TimeMLP(nn.Module):\n",
    "    '''\n",
    "    naive introduce timestep information to feature maps with mlp and add shortcut\n",
    "    '''\n",
    "    def __init__(self,embedding_dim,hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.act=nn.SiLU()\n",
    "\n",
    "        self.mlp=nn.Sequential(nn.Linear(embedding_dim,hidden_dim),\n",
    "                                self.act,\n",
    "                               nn.Linear(hidden_dim,out_dim))\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        \n",
    "        t_emb=self.mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        # print('t_emb shape', t_emb.shape, 'x.shape', x.shape)\n",
    "        x=x+t_emb\n",
    "        return self.act(x)\n",
    "\n",
    "class TargetMLP_and_CONV(nn.Module):\n",
    "    def __init__(self,embedding_dim,hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.act=nn.LeakyReLU() \n",
    "\n",
    "        self.size_map = {\n",
    "            32: 2*28,\n",
    "            64: 2*14,\n",
    "            128: 2*7,\n",
    "            256: 7\n",
    "        }\n",
    "\n",
    "        self.mlp=nn.Sequential(nn.Linear(embedding_dim,hidden_dim),\n",
    "                                self.act,\n",
    "                                nn.Linear(hidden_dim, self.size_map[out_dim]**2))\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_dim+1, out_dim, 3, 1, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        y_emb = self.mlp(y).unsqueeze(1).view(-1, 1, self.size_map[self.out_dim], self.size_map[self.out_dim])\n",
    "        # print('y_emb shape', y_emb.shape, 'x.shape', x.shape, 'out_dim', self.out_dim, 'hidden_dim', self.hidden_dim, 'embedding_dim', self.embedding_dim)\n",
    "        x=torch.cat([x,y_emb],dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.act(x)\n",
    "        \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,time_embedding_dim, target_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv0=nn.Sequential(*[ResidualBottleneck(in_channels,in_channels) for i in range(3)],\n",
    "                                    ResidualBottleneck(in_channels,out_channels//2))\n",
    "\n",
    "        self.time_mlp=TimeMLP(embedding_dim=time_embedding_dim, hidden_dim=out_channels, out_dim=out_channels//2)\n",
    "        self.target_mlp = TargetMLP_and_CONV(embedding_dim=target_embedding_dim, \n",
    "                                    hidden_dim = out_channels, out_dim=out_channels//2)\n",
    "        self.conv1=ResidualDownsample(out_channels//2,out_channels)\n",
    "    \n",
    "    def forward(self,x,t=None, y=None):\n",
    "        # print('x shape', x.shape, 'y.shape', y.shape, 't.shape', t.shape)\n",
    "        x_shortcut=self.conv0(x)\n",
    "        if t is not None:\n",
    "            x=self.time_mlp(x_shortcut,t)\n",
    "        if y is not None:\n",
    "            x = self.target_mlp(x_shortcut, y)\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        return [x,x_shortcut]\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,time_embedding_dim, target_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.upsample=nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
    "        self.conv0=nn.Sequential(*[ResidualBottleneck(in_channels,in_channels) for i in range(3)],\n",
    "                                    ResidualBottleneck(in_channels,in_channels//2))\n",
    "\n",
    "        self.time_mlp=TimeMLP(embedding_dim=time_embedding_dim,hidden_dim=in_channels,out_dim=in_channels//2)\n",
    "        self.target_mlp = TargetMLP_and_CONV(embedding_dim=target_embedding_dim, \n",
    "                                    hidden_dim = in_channels, out_dim=in_channels//2)\n",
    "        self.conv1=ResidualBottleneck(in_channels//2,out_channels//2)\n",
    "\n",
    "    def forward(self,x,x_shortcut,t=None, y=None):\n",
    "        x=self.upsample(x)\n",
    "        x=torch.cat([x,x_shortcut],dim=1)\n",
    "        x=self.conv0(x)\n",
    "        if t is not None:\n",
    "            x=self.time_mlp(x,t)\n",
    "        x=self.conv1(x)\n",
    "        return x        \n",
    "\n",
    "class Unet(nn.Module):\n",
    "    '''\n",
    "    unet design with target input\n",
    "    '''\n",
    "    def __init__(self,timesteps,time_embedding_dim, in_channels=3,out_channels=2,base_dim=32,dim_mults=[2,4,8,16]):\n",
    "        super().__init__()\n",
    "        assert isinstance(dim_mults,(list,tuple))\n",
    "        assert base_dim%2==0 \n",
    "\n",
    "        channels=self._cal_channels(base_dim,dim_mults)\n",
    "\n",
    "        self.init_conv=ConvBnSiLu(in_channels,base_dim,3,1,1)\n",
    "        self.time_embedding=nn.Embedding(timesteps,time_embedding_dim)\n",
    "        targets = 10\n",
    "        target_embedding_dim = 10\n",
    "\n",
    "        self.target_embedding=nn.Embedding(targets,target_embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "        self.encoder_blocks=nn.ModuleList([EncoderBlock(c[0],c[1],time_embedding_dim, target_embedding_dim) for c in channels])\n",
    "        self.decoder_blocks=nn.ModuleList([DecoderBlock(c[1],c[0],time_embedding_dim, target_embedding_dim) for c in channels[::-1]])\n",
    "    \n",
    "        self.mid_block=nn.Sequential(*[ResidualBottleneck(channels[-1][1],channels[-1][1]) for i in range(2)],\n",
    "                                        ResidualBottleneck(channels[-1][1],channels[-1][1]//2))\n",
    "\n",
    "        self.final_conv=nn.Conv2d(in_channels=channels[0][0]//2,out_channels=out_channels,kernel_size=1)\n",
    "\n",
    "    def forward(self,x,t=None, y=None):\n",
    "        x=self.init_conv(x)\n",
    "        if t is not None:\n",
    "            t=self.time_embedding(t)\n",
    "            \n",
    "        if y is not None:\n",
    "            y = self.target_embedding(y)\n",
    "\n",
    "        encoder_shortcuts=[]\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x,x_shortcut=encoder_block(x, t, y)\n",
    "            encoder_shortcuts.append(x_shortcut)\n",
    "        \n",
    "        x=self.mid_block(x)\n",
    "\n",
    "\n",
    "        encoder_shortcuts.reverse()\n",
    "        for decoder_block,shortcut in zip(self.decoder_blocks,encoder_shortcuts):\n",
    "            x=decoder_block(x, shortcut, t, y)\n",
    "        x=self.final_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _cal_channels(self,base_dim,dim_mults):\n",
    "        dims=[base_dim*x for x in dim_mults]\n",
    "        dims.insert(0,base_dim)\n",
    "        channels=[]\n",
    "        for i in range(len(dims)-1):\n",
    "            channels.append((dims[i],dims[i+1])) # in_channel, out_channel\n",
    "\n",
    "        return channels\n",
    "    \n",
    "\n",
    "class ImageDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        time_embedding_dim=64,\n",
    "        epsilon=0.008,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.time_embedding_dim = time_embedding_dim\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.model = Unet(\n",
    "            timesteps = timesteps,\n",
    "            time_embedding_dim=time_embedding_dim,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            base_dim=64,\n",
    "            dim_mults=[2, 4,]\n",
    "        )\n",
    "\n",
    "        self.shape = (-1, 1, 28, 28)\n",
    "\n",
    "        self.initialize_noise_schedule()\n",
    "\n",
    "    def initialize_noise_schedule(self, epsilon=0.008): \n",
    "        # precalculate noise schedule\n",
    "        betas = self._cosine_variance_schedule(self.timesteps, epsilon)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=-1)\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod))\n",
    "\n",
    "    \n",
    "    def forward(self, x, y, noise):\n",
    "        # x:NCHW\n",
    "        t = torch.randint(0, self.timesteps, (x.shape[0],)).to(x.device)\n",
    "        x_t = self._forward_diffusion(x, t, noise)\n",
    "        return self.model(x_t, t=t, y=y)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampling(self,n_samples,clipped_reverse_diffusion=True,device=\"mps\", y=True, tqdm_disable=True):\n",
    "        x_t = torch.randn( (n_samples, *self.shape[1:]), device=device, dtype=torch.float32)\n",
    "        hist = []\n",
    "        hist.append(x_t)\n",
    "\n",
    "        if y:\n",
    "            y = torch.randint(0, 10, (n_samples,)).to(device)\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        for i in tqdm(range(self.timesteps-1,-1,-1),desc=\"Sampling\", disable=tqdm_disable):\n",
    "            noise=torch.randn_like(x_t).to(device)\n",
    "            t=torch.tensor([i for _ in range(n_samples)], device=device, dtype=torch.long)\n",
    "\n",
    "            if clipped_reverse_diffusion:\n",
    "                x_t=self._reverse_diffusion_with_clip(x_t, y, t, noise)\n",
    "            else:\n",
    "                x_t=self._reverse_diffusion(x_t, y, t, noise)\n",
    "\n",
    "            hist.append(x_t)\n",
    "\n",
    "        # x_t=(x_t+1.)/2. #[-1,1] to [0,1]\n",
    "\n",
    "        hist=torch.stack(hist,dim=0)\n",
    "\n",
    "        return x_t, hist, y\n",
    "    \n",
    "    def _cosine_variance_schedule(self, timesteps, epsilon=0.008):\n",
    "        steps = torch.linspace(0, timesteps, steps=timesteps + 1, dtype=torch.float32)\n",
    "        f_t = (\n",
    "            torch.cos(((steps / timesteps + epsilon) / (1.0 + epsilon)) * math.pi * 0.5)\n",
    "            ** 2\n",
    "        )\n",
    "        betas = torch.clip(1.0 - f_t[1:] / f_t[:timesteps], 0.0, 0.999)\n",
    "\n",
    "        return betas\n",
    "\n",
    "    def _forward_diffusion(self, x_0, t, noise):\n",
    "        assert x_0.shape == noise.shape\n",
    "\n",
    "        A = self.sqrt_alphas_cumprod.gather(0, t).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        B = self.sqrt_one_minus_alphas_cumprod.gather(0, t).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        return A * x_0 + B * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion(self, x_t, y, t, noise):\n",
    "        \"\"\"\n",
    "        p(x_{t-1}|x_{t})-> mean,std\n",
    "\n",
    "        pred_noise-> pred_mean and pred_std\n",
    "        \"\"\"\n",
    "        pred = self.model(x_t, t=t, y=y)\n",
    "\n",
    "        alpha_t = self.alphas.gather(-1, t).reshape(x_t.shape[0], 1)  # ,1,1)\n",
    "        # print('alpha_t', alpha_t.shape)\n",
    "        alpha_t_cumprod = self.alphas_cumprod.gather(-1, t).reshape(\n",
    "            x_t.shape[0], 1\n",
    "        )  # ,1,1)\n",
    "        beta_t = self.betas.gather(-1, t).reshape(x_t.shape[0], 1)  # ,1,1)\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod.gather(\n",
    "            -1, t\n",
    "        ).reshape(\n",
    "            x_t.shape[0], 1\n",
    "        )  # ,1,1)\n",
    "        mean = (1.0 / torch.sqrt(alpha_t)) * (\n",
    "            x_t - ((1.0 - alpha_t) / sqrt_one_minus_alpha_cumprod_t) * pred\n",
    "        )\n",
    "\n",
    "        if t.min() > 0:\n",
    "            alpha_t_cumprod_prev = self.alphas_cumprod.gather(-1, t - 1).reshape(\n",
    "                x_t.shape[0], 1\n",
    "            )  # ,1,1)\n",
    "            std = torch.sqrt(\n",
    "                beta_t * (1.0 - alpha_t_cumprod_prev) / (1.0 - alpha_t_cumprod)\n",
    "            )\n",
    "        else:\n",
    "            std = 0.0\n",
    "\n",
    "        return mean + std * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion_with_clip(self, x_t, y, t, noise): \n",
    "        '''\n",
    "        p(x_{0}|x_{t}),q(x_{t-1}|x_{0},x_{t})->mean,std\n",
    "\n",
    "        pred_noise -> pred_x_0 (clip to [-1.0,1.0]) -> pred_mean and pred_std\n",
    "        '''\n",
    "        pred=self.model(x_t,t=t,y=y)\n",
    "        alpha_t=self.alphas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        alpha_t_cumprod=self.alphas_cumprod.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        beta_t=self.betas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        \n",
    "        x_0_pred=torch.sqrt(1. / alpha_t_cumprod)*x_t-torch.sqrt(1. / alpha_t_cumprod - 1.)*pred\n",
    "        x_0_pred.clamp_(-1., 1.)\n",
    "\n",
    "        if t.min()>0:\n",
    "            alpha_t_cumprod_prev=self.alphas_cumprod.gather(-1,t-1).reshape(x_t.shape[0],1,1,1)\n",
    "            mean= (beta_t * torch.sqrt(alpha_t_cumprod_prev) / (1. - alpha_t_cumprod))*x_0_pred +\\\n",
    "                 ((1. - alpha_t_cumprod_prev) * torch.sqrt(alpha_t) / (1. - alpha_t_cumprod))*x_t\n",
    "\n",
    "            std=torch.sqrt(beta_t*(1.-alpha_t_cumprod_prev)/(1.-alpha_t_cumprod))\n",
    "        else:\n",
    "            mean=(beta_t / (1. - alpha_t_cumprod))*x_0_pred #alpha_t_cumprod_prev=1 since 0!=1\n",
    "            std=0.0\n",
    "\n",
    "        return mean+std*noise \n",
    "\n",
    "# make pl model\n",
    "class ImageDiffusionModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        criteria=nn.MSELoss(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lr = kwargs.get(\"LEARNING_RATE\", 0.001)\n",
    "        self.model = ImageDiffusion(\n",
    "            timesteps=kwargs.get(\"TIMESTEPS\", 300),\n",
    "            time_embedding_dim=kwargs.get(\"TIME_EMBEDDING_DIM\", 8),\n",
    "            # target_embedding_dim=kwargs.get(\"TARGET_EMBEDDING_DIM\", 8),\n",
    "            epsilon=kwargs.get(\"EPSILON\", 0.008),\n",
    "        )\n",
    "        \n",
    "        self.criteria = criteria\n",
    "        self.noise_mult = 1.3\n",
    "    \n",
    "    def forward(self, data,):\n",
    "        \n",
    "        x, y = data\n",
    "        if self.current_epoch < 0:\n",
    "            y = None\n",
    "        noise = torch.randn_like(x) * self.noise_mult\n",
    "        pred_noise = self.model(x, y, noise)\n",
    "        return noise, pred_noise\n",
    "    \n",
    "    def _common_step(self, batch, stage='train'):\n",
    "        noise, pred_noise = self.forward(batch)\n",
    "        return self.criteria(pred_noise, noise) \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, stage=\"train\")\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=False,)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 1)  # clip gradients\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, stage=\"val\")\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=True, on_epoch=False,)\n",
    "\n",
    "        # check_noise_level\n",
    "        if batch_idx == 0 and self.current_epoch == 0:\n",
    "            self.check_noise_level(batch, N=12)\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        if self.current_epoch % 3 == 0:\n",
    "\n",
    "            # Simplify y value assignment\n",
    "            y = self.current_epoch >= 0\n",
    "            \n",
    "            # Sample from model\n",
    "            sample, hist, y_flags = self.model.sampling(6, y=y)\n",
    "            \n",
    "\n",
    "\n",
    "            # Ensure hist is not empty and prepare it for plotting\n",
    "            \n",
    "            hist = hist[::len(hist) // 4].squeeze().cpu()\n",
    "            # Create a figure with subplots\n",
    "            rows, cols = hist.shape[:2]\n",
    "            fig, ax = plt.subplots(rows, cols, figsize=(10, 10 * rows / cols))\n",
    "            \n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    ax[i, j].imshow(hist[i, j], cmap='gray')\n",
    "                    ax[i, j].axis('off')\n",
    "            \n",
    "            # Set top row titles to y_flags\n",
    "            if y_flags is not None and len(y_flags) == cols:\n",
    "                for i, flag in enumerate(y_flags):\n",
    "                    ax[0, i].set_title(f'y={flag.item()}')\n",
    "                    \n",
    "            self.logger.experiment.add_figure(f'hist', fig,  global_step=self.global_step)\n",
    "            plt.close()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, stage=\"test\")\n",
    "        self.log('test_loss', loss, prog_bar=True, on_step=True, on_epoch=False,)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def check_noise_level(self, batch, N=8):\n",
    "        x, y = batch\n",
    "        x = x[:N]\n",
    "        y = y[:N]\n",
    "\n",
    "        x = x[2:3]\n",
    "        x = x.repeat(N, 1, 1, 1)\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        # make t an interger linspace\n",
    "        t = torch.linspace(0, self.model.timesteps, steps=N, dtype=torch.long).to('mps')\n",
    "        x_t = self.model._forward_diffusion(x, t, noise)\n",
    "\n",
    "        grid = torchvision.utils.make_grid(x_t, nrow=3)\n",
    "        self.logger.experiment.add_image(\"x_t\", grid, global_step=self.global_step)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        # decrease lr by 0.1 every 10 epochs\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=2/3, verbose=False)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "\n",
    "dm = MNISTDataModule(BATCH_SIZE=256, )\n",
    "dm.setup() # max_samples=10000 doesn't work for some reason\n",
    "\n",
    "plModule = ImageDiffusionModule(\n",
    "    LEARNING_RATE = 0.005,\n",
    "    TIMESTEPS=1000\n",
    ")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\"logs\", name=\"diffusion\")\n",
    "trainer = pl.Trainer(max_epochs=100,\n",
    "                     logger=logger,)\n",
    "trainer.fit(plModule, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.6031, -0.5111, -0.5810, -0.5736, -0.5725, -0.5964, -0.4619,\n",
       "            -0.5452, -0.4080, -0.5840, -0.3527, -0.6432, -0.2352, -0.2938,\n",
       "            -0.6069, -0.2653, -0.3614, -0.4836, -0.4198, -0.4069, -0.2110,\n",
       "            -0.2891, -0.4566, -0.5402, -0.5427, -0.4958, -0.3208, -0.4599],\n",
       "           [-0.5617, -0.4251, -0.5689, -0.4858, -0.4927, -0.4907, -0.2659,\n",
       "            -0.3404, -0.5164, -0.5120, -0.4208, -0.5583, -0.3858, -0.5930,\n",
       "            -0.5716, -0.5775, -0.4512, -0.5906, -0.4032, -0.3555, -0.0831,\n",
       "            -0.4779, -0.6165, -0.2711, -0.5379, -0.4170, -0.5872, -0.4742],\n",
       "           [-0.4824, -0.6416, -0.5671, -0.4449, -0.6152, -0.4770, -0.3928,\n",
       "            -0.4959, -0.4872, -0.3870, -0.5526, -0.5072, -0.2654, -0.3218,\n",
       "            -0.6294, -0.6072, -0.4897, -0.3532, -0.5064, -0.4888, -0.2670,\n",
       "            -0.4021, -0.4943, -0.1927, -0.5644, -0.4413, -0.3854, -0.4461],\n",
       "           [-0.5111, -0.4337, -0.4975, -0.3743, -0.6199, -0.5468, -0.3413,\n",
       "            -0.4113, -0.5347, -0.5025, -0.2719, -0.3179, -0.5136, -0.5097,\n",
       "            -0.6583, -0.5950, -0.4551, -0.4311, -0.5724, -0.4820, -0.5191,\n",
       "            -0.5186, -0.3733, -0.5378, -0.5915, -0.4644, -0.4757, -0.5062],\n",
       "           [-0.4232, -0.6821, -0.5349, -0.5194, -0.3439, -0.5418, -0.5289,\n",
       "            -0.5041, -0.5752, -0.1446, -0.2172, -0.4811,  0.0372, -0.0525,\n",
       "            -0.1895, -0.3659, -0.4384, -0.3848, -0.4615, -0.5852, -0.5068,\n",
       "            -0.3192, -0.4691, -0.0853, -0.4589, -0.5154, -0.3845, -0.4477],\n",
       "           [-0.1620, -0.4922, -0.5493, -0.4805, -0.3335, -0.4753, -0.3797,\n",
       "            -0.4543, -0.4430,  0.2182, -0.3439, -0.0648, -0.1734, -0.0424,\n",
       "            -0.1190, -0.3073,  0.1332, -0.4005, -0.0770, -0.3810, -0.6421,\n",
       "            -0.4941, -0.4781, -0.4159, -0.4518, -0.6005, -0.5407, -0.5143],\n",
       "           [-0.6202, -0.6469, -0.3325, -0.4227, -0.3412, -0.4488, -0.2689,\n",
       "            -0.4185, -0.1130,  0.0532, -0.3018, -0.2514, -0.4852, -0.3815,\n",
       "            -0.4250, -0.3232, -0.1730, -0.2135, -0.2484, -0.5906, -0.6076,\n",
       "            -0.1847, -0.4695, -0.4623, -0.4952, -0.4686, -0.4661, -0.5470],\n",
       "           [-0.5561, -0.6722, -0.5006, -0.5435, -0.4085, -0.5284, -0.4592,\n",
       "            -0.0154,  0.1152,  0.1673, -0.4456, -0.4040, -0.4114, -0.6393,\n",
       "            -0.4357, -0.1990, -0.0169,  0.2549, -0.1310, -0.5244, -0.4294,\n",
       "            -0.5067, -0.4627, -0.4547, -0.5202, -0.5127, -0.5216, -0.5062],\n",
       "           [-0.4634, -0.5749, -0.4518, -0.5113, -0.4372, -0.2061, -0.2794,\n",
       "            -0.0512, -0.1345, -0.0188, -0.4505, -0.3470, -0.2715, -0.2189,\n",
       "            -0.5009, -0.3442,  0.0253,  0.2129, -0.1680, -0.6131, -0.5046,\n",
       "            -0.5202, -0.4692, -0.4385, -0.4851, -0.5285, -0.5255, -0.5286],\n",
       "           [-0.3652, -0.1435, -0.3529, -0.4785, -0.5837, -0.5238, -0.0708,\n",
       "            -0.2013, -0.2528, -0.3255, -0.1279, -0.2651, -0.3876, -0.6702,\n",
       "            -0.3863, -0.2383, -0.0741, -0.1909, -0.1314, -0.5707, -0.4729,\n",
       "            -0.4925, -0.5114, -0.5137, -0.5106, -0.5241, -0.5087, -0.5394],\n",
       "           [-0.3303, -0.4757, -0.4970, -0.5677, -0.4976, -0.5499, -0.3238,\n",
       "             0.0687, -0.0929, -0.0613,  0.1394, -0.0762, -0.3171, -0.4425,\n",
       "            -0.2399,  0.2217,  0.0761, -0.2299, -0.3342, -0.4662, -0.4934,\n",
       "            -0.4886, -0.4907, -0.4862, -0.5297, -0.5192, -0.5141, -0.5122],\n",
       "           [-0.6399, -0.5324, -0.5009, -0.3454, -0.5383, -0.3630, -0.5322,\n",
       "            -0.1211, -0.2114, -0.1311,  0.2169, -0.4608, -0.3139, -0.4269,\n",
       "             0.1319,  0.3226,  0.2133, -0.0135, -0.3575, -0.5120, -0.5015,\n",
       "            -0.5065, -0.4791, -0.4951, -0.4386, -0.5118, -0.5300, -0.5368],\n",
       "           [-0.5682, -0.5100, -0.2968, -0.4061, -0.4885, -0.4805, -0.3712,\n",
       "            -0.5522, -0.2716, -0.2780, -0.3025, -0.5239, -0.4581, -0.2964,\n",
       "             0.5637,  0.3073,  0.6001,  0.6213, -0.1288, -0.3006, -0.4958,\n",
       "            -0.5055, -0.4882, -0.5062, -0.4924, -0.5133, -0.5208, -0.5309],\n",
       "           [-0.5653, -0.3832, -0.3317, -0.3681, -0.4906, -0.4260, -0.4311,\n",
       "            -0.5449, -0.4771, -0.4147, -0.3755, -0.5914, -0.5138, -0.4614,\n",
       "            -0.1026, -0.2707,  0.0223,  0.7544,  0.7279,  0.6313, -0.1523,\n",
       "            -0.4426, -0.4985, -0.5206, -0.5131, -0.5181, -0.5105, -0.5273],\n",
       "           [-0.3797, -0.3866, -0.6673, -0.3730, -0.5157, -0.5747, -0.5130,\n",
       "            -0.5805, -0.5890, -0.5040, -0.5367, -0.5144, -0.5274, -0.5526,\n",
       "            -0.4635, -0.4915, -0.4985, -0.3119,  0.5804,  0.7575,  0.7347,\n",
       "             0.6834, -0.4409, -0.5175, -0.4866, -0.5091, -0.5068, -0.5113],\n",
       "           [-0.5922, -0.4059, -0.5897, -0.6301, -0.5422, -0.3634, -0.6848,\n",
       "            -0.2799, -0.4940, -0.4270, -0.5193, -0.5390, -0.5101, -0.4877,\n",
       "            -0.4990, -0.4964, -0.5016, -0.4992, -0.4797,  0.0980,  0.7589,\n",
       "             0.7629,  0.5405, -0.5157, -0.5109, -0.5170, -0.4986, -0.5175],\n",
       "           [-0.5289, -0.5996, -0.4485, -0.3671, -0.3108, -0.3632, -0.5724,\n",
       "            -0.4887, -0.4785, -0.4850, -0.5250, -0.5292, -0.5190, -0.4988,\n",
       "            -0.4823, -0.4968, -0.5120, -0.5046, -0.5000, -0.5138,  0.0659,\n",
       "             0.7646,  0.3411, -0.5214, -0.4968, -0.5206, -0.5128, -0.5092],\n",
       "           [-0.5865, -0.4988, -0.5399, -0.5675, -0.3729, -0.5317, -0.4834,\n",
       "            -0.6510, -0.5633, -0.5565, -0.4499,  0.2073,  0.5497, -0.3955,\n",
       "            -0.4959, -0.4956, -0.5061, -0.5129, -0.5085, -0.4923,  0.3712,\n",
       "             0.7552,  0.1749, -0.4936, -0.5106, -0.5127, -0.5161, -0.5266],\n",
       "           [-0.5127, -0.5446, -0.5481, -0.5422, -0.5373, -0.5463, -0.5112,\n",
       "            -0.5270, -0.5438, -0.4919,  0.6329,  0.7616,  0.7781, -0.4305,\n",
       "            -0.4942, -0.4965, -0.4981, -0.5075, -0.4998, -0.2995,  0.7447,\n",
       "             0.7208, -0.2402, -0.5095, -0.5009, -0.5200, -0.5110, -0.5226],\n",
       "           [-0.5551, -0.4725, -0.4666, -0.5298, -0.5007, -0.5258, -0.5175,\n",
       "            -0.4801, -0.4952, -0.2065,  0.7447,  0.7480, -0.0956, -0.4909,\n",
       "            -0.4969, -0.4995,  0.1657,  0.7297, -0.0746,  0.6386,  0.7422,\n",
       "             0.4443, -0.4982, -0.5037, -0.5009, -0.5122, -0.5182, -0.5128],\n",
       "           [-0.5275, -0.5346,  0.0844,  0.7655, -0.4967, -0.5531, -0.5116,\n",
       "            -0.4896, -0.4878, -0.2013,  0.7025,  0.0420, -0.4938, -0.4950,\n",
       "            -0.5075,  0.2931,  0.7551, -0.2018,  0.4994,  0.6830,  0.2585,\n",
       "            -0.4802, -0.4989, -0.4844, -0.5124, -0.5151, -0.5172, -0.5184],\n",
       "           [-0.5295, -0.4790,  0.5079,  0.6266, -0.4077, -0.5271, -0.5160,\n",
       "            -0.5084, -0.5197, -0.4979, -0.5148, -0.5091, -0.4939, -0.5116,\n",
       "            -0.5040,  0.6911,  0.7761, -0.2288,  0.5479, -0.2000, -0.5137,\n",
       "            -0.5035, -0.4873, -0.1493, -0.5048, -0.5084, -0.5188, -0.5166],\n",
       "           [-0.5374, -0.5373, -0.5188,  0.6223,  0.6507, -0.4831, -0.5053,\n",
       "            -0.5158, -0.5354, -0.5056, -0.5153, -0.5081, -0.5091, -0.5181,\n",
       "            -0.5199, -0.0862,  0.7277,  0.5236, -0.2907, -0.5466, -0.5180,\n",
       "            -0.5140, -0.5008,  0.0454, -0.4910, -0.5227, -0.3354, -0.5201],\n",
       "           [-0.5430, -0.5018, -0.5039, -0.2086,  0.6282, -0.1906, -0.5123,\n",
       "            -0.2090, -0.5204, -0.5147, -0.5112, -0.5157, -0.5021, -0.5343,\n",
       "            -0.5176, -0.4737,  0.2201,  0.5087, -0.4779, -0.5219, -0.5220,\n",
       "            -0.5015, -0.5002, -0.5128, -0.5047, -0.5088, -0.4997, -0.5078],\n",
       "           [-0.5400, -0.5122, -0.4894, -0.5309, -0.5088, -0.5308, -0.5295,\n",
       "             0.0373, -0.5348, -0.5053, -0.5373, -0.5122, -0.5144, -0.5079,\n",
       "            -0.5685, -0.5319, -0.5215, -0.5159, -0.5251, -0.4793, -0.5503,\n",
       "            -0.5437, -0.5099, -0.5143, -0.4863, -0.5085, -0.5074, -0.5155],\n",
       "           [-0.5101, -0.4838, -0.5133, -0.5078, -0.5278, -0.5192, -0.5201,\n",
       "            -0.5158, -0.5211, -0.5331, -0.5340, -0.5657, -0.5145, -0.5244,\n",
       "            -0.5223, -0.5463, -0.5032, -0.5302, -0.5461, -0.5129, -0.5054,\n",
       "            -0.5102, -0.4988, -0.4889, -0.5315, -0.5182, -0.5130, -0.5480],\n",
       "           [-0.5195, -0.5049, -0.5034, -0.5258, -0.4923, -0.4948, -0.5144,\n",
       "            -0.4983, -0.5507, -0.5378, -0.4979, -0.5349, -0.4934, -0.4721,\n",
       "            -0.4897, -0.4896, -0.5003, -0.5186, -0.5005, -0.5026, -0.4982,\n",
       "            -0.5219, -0.5275, -0.4891, -0.4491, -0.5193, -0.4813, -0.5405],\n",
       "           [-0.5136, -0.5389, -0.5352, -0.5644, -0.5167, -0.5038, -0.5112,\n",
       "            -0.5241, -0.5160, -0.5575, -0.5279, -0.5260, -0.4804, -0.5031,\n",
       "            -0.5047, -0.5040, -0.5161, -0.5204, -0.4991,  0.0096, -0.5203,\n",
       "            -0.5128, -0.5569, -0.5096, -0.4983, -0.5327, -0.5195, -0.5119]]]]),\n",
       " tensor([[[[[-9.4264e-01,  5.9351e-01,  1.4349e-01,  ..., -5.8361e-01,\n",
       "              6.2119e-01,  1.2361e+00],\n",
       "            [-1.4231e+00, -7.6778e-01,  1.1839e+00,  ...,  1.5480e+00,\n",
       "              3.4664e-01,  5.4304e-01],\n",
       "            [-1.1542e+00, -2.3759e-01, -1.5721e+00,  ...,  3.8890e-01,\n",
       "             -1.5063e+00,  1.0074e+00],\n",
       "            ...,\n",
       "            [ 1.0093e+00,  2.8487e-02, -4.3951e-01,  ...,  1.0693e-01,\n",
       "              6.0672e-01, -1.9526e+00],\n",
       "            [-1.2454e+00,  1.0796e+00, -6.3225e-01,  ...,  1.1164e+00,\n",
       "             -6.8243e-01,  3.1400e-01],\n",
       "            [ 1.1590e+00,  2.2755e-01,  1.2851e-01,  ...,  4.1343e-01,\n",
       "             -7.1007e-01,  1.2174e+00]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[ 2.6501e-01,  4.6275e-01, -1.5650e+00,  ...,  9.0933e-01,\n",
       "             -2.4932e-01, -6.5401e-01],\n",
       "            [ 1.3405e-01, -1.1574e+00, -5.4351e-01,  ..., -9.2064e-02,\n",
       "             -7.8273e-01,  1.5484e+00],\n",
       "            [-1.9642e+00,  8.6254e-01, -1.5794e-01,  ..., -6.3018e-01,\n",
       "             -6.7409e-01,  2.0897e+00],\n",
       "            ...,\n",
       "            [ 2.5803e-01, -2.1628e+00,  2.4372e-01,  ...,  9.3614e-01,\n",
       "             -1.7747e+00, -2.5581e-01],\n",
       "            [ 2.3235e+00, -1.0967e+00, -5.9596e-02,  ..., -1.9056e+00,\n",
       "             -4.5392e-01,  1.3792e+00],\n",
       "            [ 6.9194e-02, -2.1748e+00, -8.5008e-01,  ...,  4.8897e-01,\n",
       "              9.7147e-01, -7.5469e-01]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[-6.5964e-01,  1.0506e+00, -7.6924e-01,  ...,  9.3114e-01,\n",
       "             -1.6070e+00, -1.9343e-03],\n",
       "            [ 3.4786e-02, -3.1150e-01, -2.7545e-01,  ..., -5.5080e-01,\n",
       "             -1.7025e+00,  2.1707e+00],\n",
       "            [-6.0673e-01, -8.5273e-01, -1.1048e+00,  ..., -1.6626e+00,\n",
       "              3.4166e-01,  1.7931e+00],\n",
       "            ...,\n",
       "            [-8.6365e-02, -1.8164e-01, -1.3194e-01,  ...,  5.2951e-01,\n",
       "             -2.3577e-01, -1.6423e+00],\n",
       "            [ 1.2198e+00, -8.1022e-01,  1.2201e+00,  ..., -3.6281e-01,\n",
       "              8.0387e-01,  1.3483e+00],\n",
       "            [-1.6720e+00, -6.6373e-01, -1.3542e+00,  ..., -1.0718e-01,\n",
       "              1.7680e+00, -5.4809e-01]]]],\n",
       " \n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       " \n",
       "         [[[[-6.2827e-01, -5.1000e-01, -6.0592e-01,  ..., -5.0659e-01,\n",
       "             -2.9661e-01, -4.6548e-01],\n",
       "            [-5.8391e-01, -4.2956e-01, -6.0184e-01,  ..., -4.0081e-01,\n",
       "             -6.0820e-01, -4.5762e-01],\n",
       "            [-4.7408e-01, -6.8528e-01, -6.0280e-01,  ..., -4.3799e-01,\n",
       "             -3.5625e-01, -4.2801e-01],\n",
       "            ...,\n",
       "            [-5.2084e-01, -4.9937e-01, -5.1283e-01,  ..., -5.2065e-01,\n",
       "             -5.0983e-01, -5.5945e-01],\n",
       "            [-5.0990e-01, -5.1407e-01, -4.9064e-01,  ..., -5.3726e-01,\n",
       "             -4.7822e-01, -5.4470e-01],\n",
       "            [-5.3553e-01, -5.7498e-01, -5.3448e-01,  ..., -5.3693e-01,\n",
       "             -5.1700e-01, -4.9557e-01]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[-6.1402e-01, -5.1366e-01, -5.9298e-01,  ..., -4.9699e-01,\n",
       "             -3.0707e-01, -4.5589e-01],\n",
       "            [-5.6891e-01, -4.2012e-01, -5.7982e-01,  ..., -4.0880e-01,\n",
       "             -6.0019e-01, -4.7188e-01],\n",
       "            [-4.8219e-01, -6.6732e-01, -5.7729e-01,  ..., -4.3434e-01,\n",
       "             -3.6846e-01, -4.3237e-01],\n",
       "            ...,\n",
       "            [-5.1232e-01, -4.8386e-01, -5.1736e-01,  ..., -5.2292e-01,\n",
       "             -5.1574e-01, -5.5264e-01],\n",
       "            [-5.2345e-01, -5.0777e-01, -5.0531e-01,  ..., -5.2369e-01,\n",
       "             -4.7954e-01, -5.4503e-01],\n",
       "            [-5.1550e-01, -5.4785e-01, -5.4192e-01,  ..., -5.3711e-01,\n",
       "             -5.2201e-01, -5.0901e-01]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[-6.0313e-01, -5.1105e-01, -5.8095e-01,  ..., -4.9580e-01,\n",
       "             -3.2078e-01, -4.5986e-01],\n",
       "            [-5.6175e-01, -4.2513e-01, -5.6891e-01,  ..., -4.1696e-01,\n",
       "             -5.8716e-01, -4.7425e-01],\n",
       "            [-4.8243e-01, -6.4157e-01, -5.6706e-01,  ..., -4.4131e-01,\n",
       "             -3.8539e-01, -4.4609e-01],\n",
       "            ...,\n",
       "            [-5.1007e-01, -4.8378e-01, -5.1327e-01,  ..., -5.1816e-01,\n",
       "             -5.1299e-01, -5.4803e-01],\n",
       "            [-5.1952e-01, -5.0485e-01, -5.0337e-01,  ..., -5.1926e-01,\n",
       "             -4.8132e-01, -5.4046e-01],\n",
       "            [-5.1361e-01, -5.3891e-01, -5.3517e-01,  ..., -5.3272e-01,\n",
       "             -5.1954e-01, -5.1194e-01]]]]]),\n",
       " tensor([3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plModule.model.sampling(1, y=True, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2mENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
