{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataModule params:\n",
      "\tbatch_size: 64\n",
      "\tpath: /Users/tonton/Documents/motion-synthesis/mnist_latent_diffusion/\n",
      "\trotation: 0\n",
      "\tscale: 0\n",
      "\ttranslate: (0, 0)\n",
      "\tshear: 0\n",
      "\tnormalize: (0.1307, 0.3081)\n",
      "\tbool: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type           | Params\n",
      "--------------------------------------------\n",
      "0 | model    | ImageDiffusion | 428 K \n",
      "1 | criteria | MSELoss        | 0     \n",
      "--------------------------------------------\n",
      "428 K     Trainable params\n",
      "0         Non-trainable params\n",
      "428 K     Total params\n",
      "1.715     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235bc8a064e0420f84c0841884c41f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00676846346b4696a8ba8279ee754977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from VAE.dataset import MNISTDataModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self,groups):\n",
    "        super().__init__()\n",
    "        self.groups=groups\n",
    "    def forward(self,x):\n",
    "        n,c,h,w=x.shape\n",
    "        x=x.view(n,self.groups,c//self.groups,h,w) # group\n",
    "        x=x.transpose(1,2).contiguous().view(n,-1,h,w) #shuffle\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ConvBnSiLu(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride=1,padding=0):\n",
    "        super().__init__()\n",
    "        self.module=nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size,stride=stride,padding=padding),\n",
    "                                  nn.BatchNorm2d(out_channels),\n",
    "                                  nn.SiLU(inplace=True))\n",
    "    def forward(self,x):\n",
    "        return self.module(x)\n",
    "\n",
    "class ResidualBottleneck(nn.Module):\n",
    "    '''\n",
    "    shufflenet_v2 basic unit(https://arxiv.org/pdf/1807.11164.pdf)\n",
    "    '''\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1=nn.Sequential(nn.Conv2d(in_channels//2,in_channels//2,3,1,1,groups=in_channels//2),\n",
    "                                    nn.BatchNorm2d(in_channels//2),\n",
    "                                    ConvBnSiLu(in_channels//2,out_channels//2,1,1,0))\n",
    "        self.branch2=nn.Sequential(ConvBnSiLu(in_channels//2,in_channels//2,1,1,0),\n",
    "                                    nn.Conv2d(in_channels//2,in_channels//2,3,1,1,groups=in_channels//2),\n",
    "                                    nn.BatchNorm2d(in_channels//2),\n",
    "                                    ConvBnSiLu(in_channels//2,out_channels//2,1,1,0))\n",
    "        self.channel_shuffle=ChannelShuffle(2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x1,x2=x.chunk(2,dim=1)\n",
    "        x=torch.cat([self.branch1(x1),self.branch2(x2)],dim=1)\n",
    "        x=self.channel_shuffle(x) #shuffle two branches\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResidualDownsample(nn.Module):\n",
    "    '''\n",
    "    shufflenet_v2 unit for spatial down sampling(https://arxiv.org/pdf/1807.11164.pdf)\n",
    "    '''\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1=nn.Sequential(nn.Conv2d(in_channels,in_channels,3,2,1,groups=in_channels),\n",
    "                                    nn.BatchNorm2d(in_channels),\n",
    "                                    ConvBnSiLu(in_channels,out_channels//2,1,1,0))\n",
    "        self.branch2=nn.Sequential(ConvBnSiLu(in_channels,out_channels//2,1,1,0),\n",
    "                                    nn.Conv2d(out_channels//2,out_channels//2,3,2,1,groups=out_channels//2),\n",
    "                                    nn.BatchNorm2d(out_channels//2),\n",
    "                                    ConvBnSiLu(out_channels//2,out_channels//2,1,1,0))\n",
    "        self.channel_shuffle=ChannelShuffle(2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=torch.cat([self.branch1(x),self.branch2(x)],dim=1)\n",
    "        x=self.channel_shuffle(x) #shuffle two branches\n",
    "\n",
    "        return x\n",
    "\n",
    "class TimeMLP(nn.Module):\n",
    "    '''\n",
    "    naive introduce timestep information to feature maps with mlp and add shortcut\n",
    "    '''\n",
    "    def __init__(self,embedding_dim,hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.act=nn.SiLU()\n",
    "\n",
    "        self.mlp=nn.Sequential(nn.Linear(embedding_dim,hidden_dim),\n",
    "                                self.act,\n",
    "                               nn.Linear(hidden_dim,out_dim))\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        \n",
    "        t_emb=self.mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        # print('t_emb shape', t_emb.shape, 'x.shape', x.shape)\n",
    "        x=x+t_emb\n",
    "        return self.act(x)\n",
    "\n",
    "class TargetMLP_and_CONV(nn.Module):\n",
    "    def __init__(self,embedding_dim,hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.act=nn.LeakyReLU() \n",
    "\n",
    "        self.size_map = {\n",
    "            32: 28,\n",
    "            64: 14,\n",
    "            128: 7,\n",
    "        }\n",
    "\n",
    "        self.mlp=nn.Sequential(nn.Linear(embedding_dim,hidden_dim),\n",
    "                                self.act,\n",
    "                                nn.Linear(hidden_dim, self.size_map[out_dim]**2))\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_dim+1, out_dim, 3, 1, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        y_emb = self.mlp(y).unsqueeze(1).view(-1, 1, self.size_map[self.out_dim], self.size_map[self.out_dim])\n",
    "        # print('y_emb shape', y_emb.shape, 'x.shape', x.shape, 'out_dim', self.out_dim, 'hidden_dim', self.hidden_dim, 'embedding_dim', self.embedding_dim)\n",
    "        x=torch.cat([x,y_emb],dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.act(x)\n",
    "        \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,time_embedding_dim, target_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv0=nn.Sequential(*[ResidualBottleneck(in_channels,in_channels) for i in range(3)],\n",
    "                                    ResidualBottleneck(in_channels,out_channels//2))\n",
    "\n",
    "        self.time_mlp=TimeMLP(embedding_dim=time_embedding_dim, hidden_dim=out_channels, out_dim=out_channels//2)\n",
    "        self.target_mlp = TargetMLP_and_CONV(embedding_dim=target_embedding_dim, \n",
    "                                    hidden_dim = out_channels, out_dim=out_channels//2)\n",
    "        self.conv1=ResidualDownsample(out_channels//2,out_channels)\n",
    "    \n",
    "    def forward(self,x,t=None, y=None):\n",
    "        # print('x shape', x.shape, 'y.shape', y.shape, 't.shape', t.shape)\n",
    "        x_shortcut=self.conv0(x)\n",
    "        if t is not None:\n",
    "            x=self.time_mlp(x_shortcut,t)\n",
    "        if y is not None:\n",
    "            x = self.target_mlp(x_shortcut, y)\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        return [x,x_shortcut]\n",
    "        \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,time_embedding_dim, target_embedding_dim):\n",
    "        super().__init__()\n",
    "        self.upsample=nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
    "        self.conv0=nn.Sequential(*[ResidualBottleneck(in_channels,in_channels) for i in range(3)],\n",
    "                                    ResidualBottleneck(in_channels,in_channels//2))\n",
    "\n",
    "        self.time_mlp=TimeMLP(embedding_dim=time_embedding_dim,hidden_dim=in_channels,out_dim=in_channels//2)\n",
    "        self.target_mlp = TargetMLP_and_CONV(embedding_dim=target_embedding_dim, \n",
    "                                    hidden_dim = in_channels, out_dim=in_channels//2)\n",
    "        self.conv1=ResidualBottleneck(in_channels//2,out_channels//2)\n",
    "\n",
    "    def forward(self,x,x_shortcut,t=None, y=None):\n",
    "        x=self.upsample(x)\n",
    "        x=torch.cat([x,x_shortcut],dim=1)\n",
    "        x=self.conv0(x)\n",
    "        if t is not None:\n",
    "            x=self.time_mlp(x,t)\n",
    "        if y is not None:\n",
    "            x = self.target_mlp(x_shortcut, y) \n",
    "        x=self.conv1(x)\n",
    "        return x        \n",
    "\n",
    "class Unet(nn.Module):\n",
    "    '''\n",
    "    unet design with target input\n",
    "    '''\n",
    "    def __init__(self,timesteps,time_embedding_dim, in_channels=3,out_channels=2,base_dim=32,dim_mults=[2,4,8,16]):\n",
    "        super().__init__()\n",
    "        assert isinstance(dim_mults,(list,tuple))\n",
    "        assert base_dim%2==0 \n",
    "\n",
    "        channels=self._cal_channels(base_dim,dim_mults)\n",
    "\n",
    "        self.init_conv=ConvBnSiLu(in_channels,base_dim,3,1,1)\n",
    "        self.time_embedding=nn.Embedding(timesteps,time_embedding_dim)\n",
    "        targets = 10\n",
    "        target_embedding_dim = 10\n",
    "\n",
    "        self.target_embedding=nn.Embedding(targets,target_embedding_dim)\n",
    "\n",
    "\n",
    "\n",
    "        self.encoder_blocks=nn.ModuleList([EncoderBlock(c[0],c[1],time_embedding_dim, target_embedding_dim) for c in channels])\n",
    "        self.decoder_blocks=nn.ModuleList([DecoderBlock(c[1],c[0],time_embedding_dim, target_embedding_dim) for c in channels[::-1]])\n",
    "    \n",
    "        self.mid_block=nn.Sequential(*[ResidualBottleneck(channels[-1][1],channels[-1][1]) for i in range(2)],\n",
    "                                        ResidualBottleneck(channels[-1][1],channels[-1][1]//2))\n",
    "\n",
    "        self.final_conv=nn.Conv2d(in_channels=channels[0][0]//2,out_channels=out_channels,kernel_size=1)\n",
    "\n",
    "    def forward(self,x,t=None, y=None):\n",
    "        x=self.init_conv(x)\n",
    "        if t is not None:\n",
    "            t=self.time_embedding(t)\n",
    "            \n",
    "        if y is not None:\n",
    "            y = self.target_embedding(y)\n",
    "\n",
    "        encoder_shortcuts=[]\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x,x_shortcut=encoder_block(x, t, y)\n",
    "            encoder_shortcuts.append(x_shortcut)\n",
    "        \n",
    "        x=self.mid_block(x)\n",
    "\n",
    "\n",
    "        encoder_shortcuts.reverse()\n",
    "        for decoder_block,shortcut in zip(self.decoder_blocks,encoder_shortcuts):\n",
    "            x=decoder_block(x, shortcut, t, y)\n",
    "        x=self.final_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _cal_channels(self,base_dim,dim_mults):\n",
    "        dims=[base_dim*x for x in dim_mults]\n",
    "        dims.insert(0,base_dim)\n",
    "        channels=[]\n",
    "        for i in range(len(dims)-1):\n",
    "            channels.append((dims[i],dims[i+1])) # in_channel, out_channel\n",
    "\n",
    "        return channels\n",
    "    \n",
    "\n",
    "class ImageDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000,\n",
    "        time_embedding_dim=64,\n",
    "        epsilon=0.008,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.time_embedding_dim = time_embedding_dim\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.model = Unet(\n",
    "            timesteps = timesteps,\n",
    "            time_embedding_dim=time_embedding_dim,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            base_dim=32,\n",
    "            dim_mults=[2, 4,]\n",
    "        )\n",
    "\n",
    "        self.shape = (-1, 1, 28, 28)\n",
    "\n",
    "        self.initialize_noise_schedule()\n",
    "\n",
    "    def initialize_noise_schedule(self, epsilon=0.008): \n",
    "        # precalculate noise schedule\n",
    "        betas = self._cosine_variance_schedule(self.timesteps, epsilon)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=-1)\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod))\n",
    "\n",
    "    \n",
    "    def forward(self, x, y, noise):\n",
    "        # x:NCHW\n",
    "        t = torch.randint(0, self.timesteps, (x.shape[0],)).to(x.device)\n",
    "        x_t = self._forward_diffusion(x, t, noise)\n",
    "\n",
    "        \n",
    "        return self.model(x_t, t=t, y=y)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampling(self,n_samples,clipped_reverse_diffusion=True,device=\"mps\", y=True):\n",
    "        x_t = torch.randn( (n_samples, *self.shape[1:]), device=device, dtype=torch.float32)\n",
    "        hist = []\n",
    "        hist.append(x_t)\n",
    "\n",
    "        if y:\n",
    "            y = torch.randint(0, 10, (n_samples,)).to(device)\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        for i in tqdm(range(self.timesteps-1,-1,-1),desc=\"Sampling\", disable=True):\n",
    "            noise=torch.randn_like(x_t).to(device)\n",
    "            t=torch.tensor([i for _ in range(n_samples)], device=device, dtype=torch.long)\n",
    "\n",
    "            if clipped_reverse_diffusion:\n",
    "                x_t=self._reverse_diffusion_with_clip(x_t, y, t, noise)\n",
    "            else:\n",
    "                x_t=self._reverse_diffusion(x_t, y, t, noise)\n",
    "\n",
    "            hist.append(x_t)\n",
    "\n",
    "        # x_t=(x_t+1.)/2. #[-1,1] to [0,1]\n",
    "\n",
    "        hist=torch.stack(hist,dim=0)\n",
    "\n",
    "        return x_t, hist, y\n",
    "    \n",
    "    def _cosine_variance_schedule(self, timesteps, epsilon=0.008):\n",
    "        steps = torch.linspace(0, timesteps, steps=timesteps + 1, dtype=torch.float32)\n",
    "        f_t = (\n",
    "            torch.cos(((steps / timesteps + epsilon) / (1.0 + epsilon)) * math.pi * 0.5)\n",
    "            ** 2\n",
    "        )\n",
    "        betas = torch.clip(1.0 - f_t[1:] / f_t[:timesteps], 0.0, 0.999)\n",
    "\n",
    "        return betas\n",
    "\n",
    "    def _forward_diffusion(self, x_0, t, noise):\n",
    "        assert x_0.shape == noise.shape\n",
    "\n",
    "        A = self.sqrt_alphas_cumprod.gather(0, t).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        B = self.sqrt_one_minus_alphas_cumprod.gather(0, t).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        return A * x_0 + B * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion(self, x_t, y, t, noise):\n",
    "        \"\"\"\n",
    "        p(x_{t-1}|x_{t})-> mean,std\n",
    "\n",
    "        pred_noise-> pred_mean and pred_std\n",
    "        \"\"\"\n",
    "        pred = self.model(x_t, t=t, y=y)\n",
    "\n",
    "        alpha_t = self.alphas.gather(-1, t).reshape(x_t.shape[0], 1)  # ,1,1)\n",
    "        # print('alpha_t', alpha_t.shape)\n",
    "        alpha_t_cumprod = self.alphas_cumprod.gather(-1, t).reshape(\n",
    "            x_t.shape[0], 1\n",
    "        )  # ,1,1)\n",
    "        beta_t = self.betas.gather(-1, t).reshape(x_t.shape[0], 1)  # ,1,1)\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod.gather(\n",
    "            -1, t\n",
    "        ).reshape(\n",
    "            x_t.shape[0], 1\n",
    "        )  # ,1,1)\n",
    "        mean = (1.0 / torch.sqrt(alpha_t)) * (\n",
    "            x_t - ((1.0 - alpha_t) / sqrt_one_minus_alpha_cumprod_t) * pred\n",
    "        )\n",
    "\n",
    "        if t.min() > 0:\n",
    "            alpha_t_cumprod_prev = self.alphas_cumprod.gather(-1, t - 1).reshape(\n",
    "                x_t.shape[0], 1\n",
    "            )  # ,1,1)\n",
    "            std = torch.sqrt(\n",
    "                beta_t * (1.0 - alpha_t_cumprod_prev) / (1.0 - alpha_t_cumprod)\n",
    "            )\n",
    "        else:\n",
    "            std = 0.0\n",
    "\n",
    "        return mean + std * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion_with_clip(self, x_t, y, t, noise): \n",
    "        '''\n",
    "        p(x_{0}|x_{t}),q(x_{t-1}|x_{0},x_{t})->mean,std\n",
    "\n",
    "        pred_noise -> pred_x_0 (clip to [-1.0,1.0]) -> pred_mean and pred_std\n",
    "        '''\n",
    "        pred=self.model(x_t,t=t,y=y)\n",
    "        alpha_t=self.alphas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        alpha_t_cumprod=self.alphas_cumprod.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        beta_t=self.betas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
    "        \n",
    "        x_0_pred=torch.sqrt(1. / alpha_t_cumprod)*x_t-torch.sqrt(1. / alpha_t_cumprod - 1.)*pred\n",
    "        x_0_pred.clamp_(-1., 1.)\n",
    "\n",
    "        if t.min()>0:\n",
    "            alpha_t_cumprod_prev=self.alphas_cumprod.gather(-1,t-1).reshape(x_t.shape[0],1,1,1)\n",
    "            mean= (beta_t * torch.sqrt(alpha_t_cumprod_prev) / (1. - alpha_t_cumprod))*x_0_pred +\\\n",
    "                 ((1. - alpha_t_cumprod_prev) * torch.sqrt(alpha_t) / (1. - alpha_t_cumprod))*x_t\n",
    "\n",
    "            std=torch.sqrt(beta_t*(1.-alpha_t_cumprod_prev)/(1.-alpha_t_cumprod))\n",
    "        else:\n",
    "            mean=(beta_t / (1. - alpha_t_cumprod))*x_0_pred #alpha_t_cumprod_prev=1 since 0!=1\n",
    "            std=0.0\n",
    "\n",
    "        return mean+std*noise \n",
    "\n",
    "# make pl model\n",
    "class ImageDiffusionModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        criteria=nn.MSELoss(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lr = kwargs.get(\"LEARNING_RATE\", 0.001)\n",
    "        self.model = ImageDiffusion(\n",
    "            timesteps=kwargs.get(\"TIMESTEPS\", 300),\n",
    "            time_embedding_dim=kwargs.get(\"TIME_EMBEDDING_DIM\", 8),\n",
    "            # target_embedding_dim=kwargs.get(\"TARGET_EMBEDDING_DIM\", 8),\n",
    "            epsilon=kwargs.get(\"EPSILON\", 0.008),\n",
    "        )\n",
    "        \n",
    "        self.criteria = criteria\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, y = data\n",
    "        if self.current_epoch < 0:\n",
    "            y = None\n",
    "        noise = torch.randn_like(x)\n",
    "        pred_noise = self.model(x, y, noise)\n",
    "        return noise, pred_noise\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch, stage=\"train\")\n",
    "\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
    "        return loss\n",
    "    \n",
    "    def _common_step(self, batch, stage='train'):\n",
    "        noise, pred_noise = self.forward(batch)\n",
    "        loss = self.criteria(pred_noise, noise) \n",
    "        self.log('total_loss', loss, prog_bar=True) \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, stage=\"val\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.current_epoch < 0:\n",
    "            y = False\n",
    "        else:\n",
    "            y = True\n",
    "        # sample from model\n",
    "        x_t, hist, y = self.model.sampling(6, y=y)\n",
    "        hist = hist[::len(hist)//4].squeeze().cpu()\n",
    "        # print('hist shape', hist.shape)\n",
    "        \n",
    "        # fig, ax \n",
    "        fig, ax = plt.subplots(hist.shape[0], hist.shape[1], figsize=(10, 10 * hist.shape[0]/hist.shape[1]))\n",
    "        for i in range(hist.shape[0]):\n",
    "            for j in range(hist.shape[1]):\n",
    "                ax[i, j].imshow(hist[i, j], cmap='gray')\n",
    "                ax[i, j].axis('off')\n",
    "\n",
    "        # set top row titles to y\n",
    "        for i, yi in enumerate(y):\n",
    "            ax[0, i].set_title(f'y={yi.item()}')\n",
    "        plt.savefig(f'assets/diffusion_full/{self.current_epoch}.png', dpi=400)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._common_step(batch, stage=\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        # decrease lr by 0.1 every 10 epochs\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9, verbose=False)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "\n",
    "dm = MNISTDataModule(BATCH_SIZE=64)\n",
    "dm.setup()\n",
    "\n",
    "plModule = ImageDiffusionModule(\n",
    "    LEARNING_RATE = 0.01,\n",
    "    TIMESTEPS=200\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100)\n",
    "trainer.fit(plModule, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2mENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
