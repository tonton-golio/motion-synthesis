{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved latent vectors for version 13\n",
      "Only one version found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "def find_saved_latent(path = f\"logs/VAE/train/\", cfg_name='config'):\n",
    "    \"\"\"\n",
    "    Find saved latent vectors from VAE training\n",
    "    \"\"\"\n",
    "\n",
    "    VAE_data = {}\n",
    "    for version in os.listdir(path):\n",
    "        version_num = version.split('_')[-1]\n",
    "        contents = os.listdir(f\"{path}{version}\")\n",
    "        base_path = os.path.join(path, version, )\n",
    "\n",
    "        if 'saved_latent' in contents:\n",
    "            print(f\"Found saved latent vectors for version {version_num}\")\n",
    "            cfg_file = None  # get config file\n",
    "            for file in contents:\n",
    "                if cfg_name in file and file.endswith('.yaml'):\n",
    "                    cfg_file = file\n",
    "                    break\n",
    "            \n",
    "            projection = None  # get projection image\n",
    "            for file in contents:\n",
    "                if 'projection' in file and file.endswith('.png'):\n",
    "                    projection = file\n",
    "                    break\n",
    "\n",
    "            checkpoints = glob.glob(f\"{base_path}/checkpoints/*\")  # check for checkpoints\n",
    "            saved_latent = os.listdir(os.path.join(base_path, 'saved_latent'))  # open saved_latent and check whats inside\n",
    "\n",
    "            VAE_data[version_num] = {\n",
    "                'saved_latent' : saved_latent,\n",
    "                'paths' : {\n",
    "                    'config' : os.path.join(base_path, cfg_file),\n",
    "                    'saved_latent' : os.path.join(base_path, 'saved_latent'),\n",
    "                    'projection' : os.path.join(base_path, projection),\n",
    "                    'checkpoints' : checkpoints,\n",
    "                    'log' : base_path,\n",
    "                },\n",
    "                'contents' : contents\n",
    "            }\n",
    "\n",
    "    return VAE_data\n",
    "\n",
    "def show_saved_latent_info(data, return_fig=False):\n",
    "\n",
    "    saved_latent_info = {}\n",
    "\n",
    "    for version, info in data.items():\n",
    "        saved_latent = info['saved_latent']\n",
    "        saved_latent_info[version] = {\n",
    "            'num_files' : len(saved_latent),\n",
    "            'size' : None,\n",
    "            'min' : None,\n",
    "            'max' : None,\n",
    "            'std_dev' : None,\n",
    "            'projection' : None\n",
    "        }\n",
    "\n",
    "        for file in saved_latent:\n",
    "            # get size of file\n",
    "            # get min and max values\n",
    "            # get std dev\n",
    "            pass\n",
    "\n",
    "        # projection_image = plt.imread(info['paths']['projection'])\n",
    "        saved_latent_info[version]['projection'] = info['paths']['projection']\n",
    "\n",
    "    fig, ax = plt.subplots(2, len(data), figsize=(20, 10))\n",
    "    if len(data) == 1:\n",
    "        ax = ax.reshape(2, 1)\n",
    "    for i, (version, info) in enumerate(data.items()):\n",
    "        ax[0, i].imshow(plt.imread(info['paths']['projection']))\n",
    "        ax[0, i].set_title(f\"Version {version}\")\n",
    "        ax[0, i].axis('off')\n",
    "\n",
    "        ax[1, i].text(0.5, 0.5, f\"Num Files: {saved_latent_info[version]['num_files']}\", ha='center', va='center')\n",
    "        ax[1, i].axis('off')\n",
    "    if return_fig:\n",
    "        return fig\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def latent_picker(path, cfg_name='config', show=True):\n",
    "    data = find_saved_latent(path, cfg_name)\n",
    "    # print(data)\n",
    "\n",
    "    # if the user has difficulty picking a version, show info\n",
    "    ## info to show: projection image, config file, saved_latent vectors (how many?, how big?, min/max values?, std dev?, etc.)\n",
    "    ## also show the checkpoint files\n",
    "    if len(data) == 0:\n",
    "        print(\"No saved latent vectors found.\")\n",
    "        return None, None\n",
    "    elif len(data) == 1:\n",
    "        print(\"Only one version found.\")\n",
    "        version = list(data.keys())[0]\n",
    "        return data[version], version\n",
    "    \n",
    "    else:\n",
    "        if show:\n",
    "            show_saved_latent_info(data)\n",
    "\n",
    "        # ask user for input of version number\n",
    "        print(\"Please enter the version number you would like to use: \")\n",
    "        for version in data.keys():\n",
    "            print(f\"\\t{version}\")\n",
    "        version = input('Version: ')\n",
    "\n",
    "        return data[version], version\n",
    "\n",
    "def load_latent(data_version, y_name='/y.pt'):\n",
    "    path = data_version['paths']['saved_latent']\n",
    "    z = torch.load(path + '/z.pt').to(torch.device('mps'))\n",
    "    y = torch.load(path + y_name).to(torch.device('mps'))\n",
    "    autoencoder = torch.load(path + '/model.pth').to(torch.device('mps'))\n",
    "    projector = torch.load(path + '/projector.pt')\n",
    "    projection = torch.load(path + '/projection.pt')\n",
    "\n",
    "    # load checkpoint\n",
    "    # checkpoint = torch.load(data_version['paths']['checkpoints'][0])\n",
    "    # autoencoder.load_state_dict(checkpoint['autoencoder_state_dict'])\n",
    "\n",
    "    return z, y , autoencoder, projector, projection\n",
    "\n",
    "\n",
    "VAE_version = 'VAE5'\n",
    "data_version, version = latent_picker(f'logs/{VAE_version}/train/', cfg_name='hparams')\n",
    "z, texts , autoencoder, projector, projection = load_latent(data_version, y_name='/texts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([96, 256]), torch.Size([96, 3, 250]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the encoder work as expected:  tensor(True)\n",
      "a person jogs in a straight line.                                                                                                                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx2word = np.load('../stranger_repos/HumanML3D/HumanML3D/texts_enc/simple/idx2word.npz', allow_pickle=True)['arr_0'].item()\n",
    "word2idx = np.load('../stranger_repos/HumanML3D/HumanML3D/texts_enc/simple/word2idx.npz', allow_pickle=True)['arr_0'].item()\n",
    "\n",
    "def translate(txt, idx2word):\n",
    "    return ' '.join([idx2word[i.item()] for i in txt])\n",
    "\n",
    "def translate_inv(txt, word2idx, max_len=250):\n",
    "    enc =  [word2idx[i] for i in txt.split()]\n",
    "    return torch.tensor(enc + [0] * (max_len - len(enc)))\n",
    "\n",
    "print('Does the encoder work as expected: ',(texts[0][0].detach().cpu() == translate_inv(translate(texts[0][0], idx2word), word2idx)).all())\n",
    "print(translate(texts[0][0], idx2word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 420, 66])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if VAE_version == 'VAE5':\n",
    "    reconstruction = autoencoder.model.decode(z[0].unsqueeze(0), \n",
    "                                              torch.tensor([200]).to(torch.device('mps')))\n",
    "\n",
    "elif VAE_version == 'VAE1':\n",
    "    # if VAE1\n",
    "    reconstruction = autoencoder.decode(z[0].unsqueeze(0))\n",
    "reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 22, 3)\n",
      "(200, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "from utils import plot_3d_motion_animation\n",
    "\n",
    "plot_3d_motion_animation(reconstruction[0].cpu().detach().numpy(), translate(texts[0][0], idx2word), \n",
    "                                     figsize=(10, 10), fps=20, radius=2, save_path=f\"recon.mp4\", velocity=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection shape: (96, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x35ec90e80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Projection shape:', projection.shape)\n",
    "plt.scatter(projection[:, 0], \n",
    "            projection[:, 1], \n",
    "            # c=range(len(projection)), cmap=\"tab10\", \n",
    "            alpha=0.7, s=2)\n",
    "\n",
    "\n",
    "# points = torch.tensor([\n",
    "#     [9.3, .8],\n",
    "#     [4.7,-3.2],\n",
    "#     [4, -1.3],\n",
    "#     [-2.1, -1.2]\n",
    "\n",
    "# ]).float()\n",
    "# for i, point in enumerate(points):\n",
    "#     plt.scatter(point[0], point[1], label=f\"point {i}\", s=100, marker='x', linewidths=8)\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*we should pass the text throguh a large language model first to get words like stroll and walk to be similar. () roberta-base*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class LatentSpaceDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, z, texts, batch_size=32):\n",
    "        super().__init__()\n",
    "        # print('z shape:', z.shape)  # this is (n, z_dim)\n",
    "        # print('texts shape:', texts.shape) # this is (n, 3, text_len) -> 3 is for the 3 different texts\n",
    "        self.z = z.repeat(3, 1)  # repeat z for each text\n",
    "        self.texts = texts.view(-1, texts.shape[-1])  # flatten texts\n",
    "        # print('z shape:', self.z.shape)  # this is (n*3, z_dim)\n",
    "        # print('texts shape:', self.texts.shape) # this is (n*3, text_len)\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_dim = z.shape[-1]\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        # split data into train, val, test\n",
    "        dataset = TensorDataset(self.z, self.texts)\n",
    "\n",
    "        # train/val/test split\n",
    "        n = len(dataset)\n",
    "        n_train = int(0.8 * n)\n",
    "        n_val = int(0.1 * n)\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        train_data, val_data, test_data = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type                            | Params\n",
      "----------------------------------------------------------------\n",
      "0 | model       | LatentDiffusion                 | 3.5 M \n",
      "1 | autoencoder | CascadingTransformerAutoEncoder | 17.5 M\n",
      "----------------------------------------------------------------\n",
      "21.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.1 M    Total params\n",
      "84.222    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77862e5957974d2ba0b9037d20193adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonton/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decode() missing 1 required positional argument: 'lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 360\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m    359\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# batch = next(iter(dm.train_dataloader()))\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# model(batch)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1030\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1030\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1059\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1056\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 306\u001b[0m, in \u001b[0;36mLatentDiffusionModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    303\u001b[0m pred_clean \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m-\u001b[39mpred_noise\n\u001b[1;32m    304\u001b[0m x_dirty \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m noise\n\u001b[0;32m--> 306\u001b[0m raw_reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dirty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mdecode(pred_clean)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# print('reconstruction', reconstruction.shape)\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# raw_and_recon = torch.cat([raw_reconstruction[:8], reconstruction[:8], ])\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# grid = torchvision.utils.make_grid(raw_and_recon[:16], nrow=8, normalize=True)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# self.logger.experiment.add_image('top: noisy input, bot: reconstruction', grid, global_step=self.global_step)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: decode() missing 1 required positional argument: 'lengths'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    # a simple model takes (batchsize, latent dim),\n",
    "    # performs linear layers\n",
    "    # and returns (batchsize, latent dim)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        hidden_dim,\n",
    "        nhidden=5,\n",
    "        timesteps=1000,\n",
    "        time_embedding_dim=64,\n",
    "        target_embedding_dim=5,\n",
    "        target_size=10054,\n",
    "        dp_rate=0.1,\n",
    "        verbose=False\n",
    "    ):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.time_embedding_dim = time_embedding_dim\n",
    "        self.target_embedding_dim = target_embedding_dim\n",
    "        self.time_embedding = nn.Embedding(timesteps, time_embedding_dim)\n",
    "        self.target_embedding = nn.Embedding(target_size+1, target_embedding_dim, sparse=False)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "\n",
    "        input_dim = latent_dim + time_embedding_dim + 250*4\n",
    "        out_put_dim = latent_dim\n",
    "        self.fc1 = nn.Linear(\n",
    "            input_dim, hidden_dim\n",
    "        )\n",
    "        \n",
    "        # input_dim = 11036\n",
    "        self.fc_hidden = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(nhidden)]\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "        # text transformer encoder\n",
    "        self.text_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=target_embedding_dim,\n",
    "                nhead=10,\n",
    "                dim_feedforward=512,\n",
    "                dropout=0.1,\n",
    "                activation='relu',\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=3\n",
    "        )\n",
    "        self.text_linear1 = nn.Linear(target_embedding_dim, 4)\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        if self.verbose:\n",
    "            print('x', x.shape)\n",
    "            print('y', y.shape)\n",
    "            print('t', t.shape)\n",
    "\n",
    "        t = self.time_embedding(t)  # (batchsize, time_embedding_dim)\n",
    "        if self.verbose:\n",
    "            print('t', t.shape)\n",
    "        y = self.target_embedding(y)\n",
    "        if self.verbose:\n",
    "            print('y (after target embedding)', y.shape)\n",
    "\n",
    "        y = self.text_transformer(y)\n",
    "        if self.verbose:\n",
    "            print('y', y.shape)\n",
    "        y = self.text_linear1(y)\n",
    "        y = nn.Flatten()(y)\n",
    "        if self.verbose:\n",
    "            print('y (done)', y.shape)\n",
    "            print('t', t.shape)\n",
    "            print('x', x.shape)\n",
    "        \n",
    "\n",
    "        x = torch.cat([x, t, y], dim=1)\n",
    "        if self.verbose:\n",
    "            print('after cat: x', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        if self.verbose:\n",
    "            print('x', x.shape)\n",
    "        for layer in self.fc_hidden:\n",
    "            # x = torch.relu(layer(x))\n",
    "            x = nn.LeakyReLU()(layer(x))\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LatentDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=8,\n",
    "        hidden_dim=64,\n",
    "        nhidden=3,\n",
    "        timesteps=1000,\n",
    "        time_embedding_dim=64,\n",
    "        target_embedding_dim=5,\n",
    "        epsilon=0.008,\n",
    "        dp_rate=0.1,\n",
    "        verbose=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.in_channels = latent_dim\n",
    "\n",
    "        betas = self._cosine_variance_schedule(timesteps, epsilon)\n",
    "        # print('betas', betas.shape)\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=-1)\n",
    "\n",
    "        # print('alphas_cumprod', alphas_cumprod.shape)\n",
    "\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\n",
    "            \"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod)\n",
    "        )\n",
    "\n",
    "        self.model = SimpleModel(\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            nhidden=nhidden,\n",
    "            timesteps=timesteps,\n",
    "            time_embedding_dim=time_embedding_dim,\n",
    "            target_embedding_dim=target_embedding_dim,\n",
    "            dp_rate=dp_rate,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y, noise):\n",
    "        # x:NCHW\n",
    "        t = torch.randint(0, self.timesteps, (x.shape[0],)).to(x.device)\n",
    "        # print('t from the LatentDIffusion forward', t)\n",
    "\n",
    "        x_t = self._forward_diffusion(x, t, noise)\n",
    "\n",
    "        # print('x_t', x_t.shape, )\n",
    "        # print('t', t.shape)\n",
    "        pred_noise = self.model(x_t, y, t)\n",
    "        # print('pred_noise', pred_noise.shape)\n",
    "\n",
    "        return pred_noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampling(self, n_samples, clipped_reverse_diffusion=True, device=\"cuda\"):\n",
    "        x_t = torch.randn(\n",
    "            (n_samples, self.in_channels, self.image_size, self.image_size)\n",
    "        ).to(device)\n",
    "        for i in tqdm(range(self.timesteps - 1, -1, -1), desc=\"Sampling\"):\n",
    "            noise = torch.randn_like(x_t).to(device)\n",
    "            t = torch.tensor([i for _ in range(n_samples)]).to(device)\n",
    "\n",
    "            if clipped_reverse_diffusion:\n",
    "                x_t = self._reverse_diffusion_with_clip(x_t, t, noise)\n",
    "            else:\n",
    "                x_t = self._reverse_diffusion(x_t, t, noise)\n",
    "\n",
    "        x_t = (x_t + 1.0) / 2.0  # [-1,1] to [0,1]\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def _cosine_variance_schedule(self, timesteps, epsilon=0.008):\n",
    "        steps = torch.linspace(0, timesteps, steps=timesteps + 1, dtype=torch.float32)\n",
    "        f_t = (\n",
    "            torch.cos(((steps / timesteps + epsilon) / (1.0 + epsilon)) * math.pi * 0.5)\n",
    "            ** 2\n",
    "        )\n",
    "        betas = torch.clip(1.0 - f_t[1:] / f_t[:timesteps], 0.0, 0.999)\n",
    "\n",
    "        return betas\n",
    "\n",
    "    def _forward_diffusion(self, x_0, t, noise):\n",
    "        # print('x_0', x_0.shape)\n",
    "        # print('noise', noise.shape)\n",
    "        # print('t', t)\n",
    "\n",
    "        assert x_0.shape == noise.shape\n",
    "        # print('self.sqrt_alphas_cumprod.gather(t)', self.sqrt_alphas_cumprod.gather(0,t).shape)\n",
    "        # q(x_{t}|x_{t-1})\n",
    "\n",
    "        A = self.sqrt_alphas_cumprod.gather(0, t).unsqueeze(1)\n",
    "        B = self.sqrt_one_minus_alphas_cumprod.gather(0, t).unsqueeze(1)\n",
    "        # print('A', A.shape)\n",
    "        # print('B', B.shape)\n",
    "        # print('noise', noise.shape)\n",
    "        # print('x_0', x_0.shape)\n",
    "        return A * x_0 + B * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reverse_diffusion(self, x_t, y, t, noise):\n",
    "        \"\"\"\n",
    "        p(x_{t-1}|x_{t})-> mean,std\n",
    "\n",
    "        pred_noise-> pred_mean and pred_std\n",
    "        \"\"\"\n",
    "        pred = self.model(x_t, y, t)\n",
    "\n",
    "        alpha_t = self.alphas.gather(-1, t).reshape(x_t.shape[0], 1)  # ,1,1)\n",
    "        # print('alpha_t', alpha_t.shape)\n",
    "        alpha_t_cumprod = self.alphas_cumprod.gather(-1, t).reshape(\n",
    "            x_t.shape[0], 1\n",
    "        )  # ,1,1)\n",
    "        beta_t = self.betas.gather(-1, t).reshape(x_t.shape[0], 1)  # ,1,1)\n",
    "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod.gather(\n",
    "            -1, t\n",
    "        ).reshape(\n",
    "            x_t.shape[0], 1\n",
    "        )  # ,1,1)\n",
    "        mean = (1.0 / torch.sqrt(alpha_t)) * (\n",
    "            x_t - ((1.0 - alpha_t) / sqrt_one_minus_alpha_cumprod_t) * pred\n",
    "        )\n",
    "\n",
    "        if t.min() > 0:\n",
    "            alpha_t_cumprod_prev = self.alphas_cumprod.gather(-1, t - 1).reshape(\n",
    "                x_t.shape[0], 1\n",
    "            )  # ,1,1)\n",
    "            std = torch.sqrt(\n",
    "                beta_t * (1.0 - alpha_t_cumprod_prev) / (1.0 - alpha_t_cumprod)\n",
    "            )\n",
    "        else:\n",
    "            std = 0.0\n",
    "\n",
    "        return mean + std * noise\n",
    "\n",
    "# make pl model\n",
    "class LatentDiffusionModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder,\n",
    "        verbose = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        self.lr = kwargs.get(\"lr\", 0.001)\n",
    "        self.model = LatentDiffusion(\n",
    "            latent_dim=kwargs.get(\"latent_dim\", 8),\n",
    "            hidden_dim=kwargs.get(\"hidden_dim\", 64),\n",
    "            nhidden=kwargs.get(\"nhidden\", 5),\n",
    "            timesteps=kwargs.get(\"timesteps\", 100),\n",
    "            time_embedding_dim=kwargs.get(\"time_embedding_dim\", 8),\n",
    "            epsilon=kwargs.get(\"epsilon\", 0.008),\n",
    "            target_embedding_dim=kwargs.get(\"target_embedding_dim\", 8),\n",
    "            dp_rate=kwargs.get(\"dp_rate\", 0.1),\n",
    "            verbose=verbose\n",
    "        )\n",
    "        self.noise_multiplier = kwargs.get(\"noise_multiplier\", .1)\n",
    "        # self.save_hyperparameters()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.autoencoder.eval()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, y = data\n",
    "        if self.verbose:\n",
    "            print('x', x.shape)\n",
    "            print('y', y.shape)\n",
    "        noise = torch.randn_like(x) * self.noise_multiplier\n",
    "        return self.model(x, y, noise), noise\n",
    "    \n",
    "    def _reverse_diffusion(self, x_t, y, t):\n",
    "        noise = torch.randn_like(x_t)\n",
    "        return self.model._reverse_diffusion(x_t, y, t, noise)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pred_noise, noise = self.forward(batch)\n",
    "        # print('pred_noise', pred_noise.shape)\n",
    "        loss = nn.functional.mse_loss(pred_noise, noise)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pred_noise, noise = self.forward(batch)\n",
    "        loss = nn.functional.mse_loss(pred_noise, noise)\n",
    "\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        if batch_idx == 0 and self.autoencoder is not None:\n",
    "            with torch.no_grad():\n",
    "                # make image by decoding latent space\n",
    "                x, y = batch\n",
    "                # print(x.shape, y.shape)\n",
    "                pred_clean = x-pred_noise\n",
    "                x_dirty = x + noise\n",
    "\n",
    "                raw_reconstruction = self.autoencoder.decode(x_dirty)\n",
    "                reconstruction = self.autoencoder.decode(pred_clean)\n",
    "                # print('reconstruction', reconstruction.shape)\n",
    "\n",
    "                # raw_and_recon = torch.cat([raw_reconstruction[:8], reconstruction[:8], ])\n",
    "\n",
    "                # grid = torchvision.utils.make_grid(raw_and_recon[:16], nrow=8, normalize=True)\n",
    "                # self.logger.experiment.add_image('top: noisy input, bot: reconstruction', grid, global_step=self.global_step)\n",
    "\n",
    "                plot_3d_motion_animation(raw_reconstruction[0].cpu().detach().numpy(), translate(y[0], idx2word), \n",
    "                                     figsize=(10, 10), fps=20, radius=2, save_path=f\"recon_dirty.mp4\", velocity=False)\n",
    "                plt.close()\n",
    "\n",
    "                plot_3d_motion_animation(reconstruction[0].cpu().detach().numpy(), translate(y[0], idx2word), \n",
    "                                     figsize=(10, 10), fps=20, radius=2, save_path=f\"recon_clean.mp4\", velocity=False)\n",
    "                plt.close()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pred_noise, noise = self.forward(batch)\n",
    "        loss = nn.functional.mse_loss(pred_noise, noise) / self.noise_multiplier\n",
    "        self.log(\n",
    "            \"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "data_module = LatentSpaceDataModule(z, texts, batch_size=4)\n",
    "data_module.setup()\n",
    "\n",
    "model = LatentDiffusionModel(\n",
    "    latent_dim=data_module.latent_dim,\n",
    "    hidden_dim=512,\n",
    "    nhidden=5,\n",
    "    timesteps=1000,\n",
    "    time_embedding_dim=12,\n",
    "    target_embedding_dim=100,\n",
    "    epsilon=0.0008,\n",
    "    dp_rate=0.1,\n",
    "    autoencoder=autoencoder.model if VAE_version == 'VAE5' else autoencoder,\n",
    "    lr = 0.0001,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "# train\n",
    "trainer = pl.Trainer(max_epochs=100, accelerator='mps')\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# batch = next(iter(dm.train_dataloader()))\n",
    "\n",
    "# model(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_enc_input torch.Size([1, 250])\n",
      "noisy_latent torch.Size([1, 256])\n",
      "tensor(-174.4735, device='mps:0')\n",
      "tensor(-174.4735, device='mps:0')\n",
      "tensor(-174.3026, device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/y8d9ljs52g7bqbgnrq59ltv40000gn/T/ipykernel_44151/12557977.py:229: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if t.min() > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 420, 22, 3])\n",
      "(420, 22, 3)\n",
      "(420, 22, 3)\n",
      "(420, 22, 3)\n",
      "(420, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "# predict text input\n",
    "text_input = 'jumping jacks'\n",
    "\n",
    "text_enc_input = translate_inv(text_input, word2idx).unsqueeze(0)\n",
    "noisy_latent = (torch.randn_like(z[0]) * 8.0).unsqueeze(0)\n",
    "print('text_enc_input', text_enc_input.shape)\n",
    "print('noisy_latent', noisy_latent.shape)\n",
    "# pred_noise, noise = model((noisy_latent.unsqueeze(0), text_enc_input.unsqueeze(0)))\n",
    "print(noisy_latent.sum())\n",
    "# # subtact pred\n",
    "t = 9\n",
    "out = noisy_latent.clone().to(torch.device('mps'))\n",
    "for i in range(t, 1, -1):\n",
    "    print(i, end='\\r')\n",
    "    out = model._reverse_diffusion(out.to(torch.device('mps')),\n",
    "                                   text_enc_input.to(torch.device('mps')),\n",
    "                                   torch.tensor([i]).to(torch.device('mps')) )\n",
    "    # print(noisy_latent.shape)\n",
    "\n",
    "print(noisy_latent.sum())\n",
    "print(out.sum())\n",
    "# decode\n",
    "reconstruction = autoencoder.decode(out)\n",
    "recon_noisy = autoencoder.decode(noisy_latent.to(torch.device('mps')))\n",
    "print(reconstruction.shape)\n",
    "\n",
    "plot_3d_motion_animation(reconstruction[0].cpu().detach().numpy(), text_input,\n",
    "                                        figsize=(10, 10), fps=20, radius=2, save_path=f\"recon_text.mp4\", velocity=False)\n",
    "plt.close()\n",
    "\n",
    "plot_3d_motion_animation(recon_noisy[0].cpu().detach().numpy(), text_input,\n",
    "                                        figsize=(10, 10), fps=20, radius=2, save_path=f\"recon_text_noisy.mp4\", velocity=False)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([i]).to(noisy_latent.device).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2mENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
