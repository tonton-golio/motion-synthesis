{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 16429, max length: 469, size: 1737.26 MB\n",
      "Number of sequences: 1046, max length: 199, size: 110.61 MB\n",
      "Number of sequences: 3064, max length: 343, size: 324.00 MB\n"
     ]
    }
   ],
   "source": [
    "from modules.data_modules import MotionDataModule\n",
    "\n",
    "\n",
    "\n",
    "cfg.DATA.batch_size = 128\n",
    "\n",
    "dm = MotionDataModule(**cfg.DATA, tiny=False)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_len': 420,\n",
       " 'batch_size': 128,\n",
       " 'file_list_paths': {'_train': '../../data/HumanML3D/HumanML3D/train_cleaned.txt',\n",
       "  '_val': '../../data/HumanML3D/HumanML3D/val_cleaned.txt',\n",
       "  '_test': '../../data/HumanML3D/HumanML3D/test_cleaned.txt'},\n",
       " '_motion_path': '../../data/HumanML3D/HumanML3D/new_joints'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.train_dataloader()))\n",
    "motion_seq, text_enc = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 420, 22, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently the dataloader takes 16 seconds to load a batch...\n",
    "# this is too slow for training\n",
    "# to speed it up; we need to make changes\n",
    "# i now keep all the data in memory, and load it from there\n",
    "# this now takes 6 seconds to load a batch\n",
    "# this is still too slow\n",
    "# i need to speed it up further\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([128, 420, 22, 3]), torch.float32),\n",
       " (torch.Size([128, 3, 250]), torch.int64)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(b.shape, b.dtype) for b in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 420, 22, 3])\n",
      "torch.Size([128, 420, 66])\n",
      "torch.Size([128, 420, 66])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def lengths_to_mask(lengths: List[int],\n",
    "                    device: torch.device,\n",
    "                    max_len: int = None) -> Tensor:\n",
    "    lengths = torch.tensor(lengths, device=device)\n",
    "    max_len = max_len if max_len else max(lengths)\n",
    "    mask = torch.arange(max_len, device=device).expand(\n",
    "        len(lengths), max_len) < lengths.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.latent_size = 1\n",
    "        self.latent_dim = d_model\n",
    "        \n",
    "        # self.query_pos_encoder = nn.Embedding(1000, d_model)\n",
    "\n",
    "        self.global_motion_token = nn.Parameter(\n",
    "                torch.randn(self.latent_size * 2, self.latent_dim))\n",
    "\n",
    "\n",
    "        self.skel_enc = nn.Linear(66, d_model)\n",
    "\n",
    "        encoder_layer =  nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, \n",
    "                nhead=nhead, \n",
    "                dim_feedforward=dim_feedforward, \n",
    "                dropout=dropout, \n",
    "                activation=activation, \n",
    "                batch_first=False\n",
    "                )\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            norm=encoder_norm,\n",
    "            num_layers=num_layers)\n",
    "        \n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            activation=activation, \n",
    "            batch_first=False\n",
    "            )\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            norm=decoder_norm,\n",
    "            num_layers=num_layers)\n",
    "        \n",
    "        self.final_layer = nn.Linear(d_model, 66)\n",
    "        \n",
    "    def forward(self, src: Tensor):\n",
    "        dist, lengths = self.encode(src)\n",
    "        z = self.reparameterize(dist[:1], dist[1:])\n",
    "        output = self.decode(z, lengths)\n",
    "        return output\n",
    "\n",
    "    def encode(self, src):\n",
    "        batch_size = src.shape[0]\n",
    "        lengths = [len(feature) for feature in src]\n",
    "        x = src.permute(1, 0, 2)  # (B, S, F) -> (S, B, F)\n",
    "        device = x.device\n",
    "        mask = lengths_to_mask(lengths, device)\n",
    "        dist = torch.tile(self.global_motion_token[:, None, :], (1, batch_size, 1))\n",
    "\n",
    "        dist_masks = torch.ones((batch_size, dist.shape[0]),\n",
    "                                dtype=bool,\n",
    "                                device=x.device)\n",
    "        aug_mask = torch.cat((dist_masks, mask), 1)\n",
    "\n",
    "        # adding the embedding token for all sequences\n",
    "        x = self.skel_enc(x)\n",
    "        xseq = torch.cat((dist, x), 0)\n",
    "        # print(xseq.shape)\n",
    "        # # xseq = self.query_pos_encoder(xseq)\n",
    "        # print(xseq.shape)\n",
    "\n",
    "        # xseq = self.query_pos_encoder(xseq)\n",
    "        dist = self.encoder(xseq,\n",
    "                            src_key_padding_mask=~aug_mask)[:dist.shape[0]]\n",
    "\n",
    "        return dist, lengths\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z: Tensor, lengths: List[int]):\n",
    "        # print('decoding')\n",
    "        mask = lengths_to_mask(lengths, z.device)\n",
    "        bs, nframes = mask.shape\n",
    "\n",
    "        queries = torch.zeros(nframes, bs, self.latent_dim, device=z.device)\n",
    "\n",
    "\n",
    "        # print(z.shape)\n",
    "        # print(queries.shape)\n",
    "        xseq = torch.cat((z, queries), axis=0)\n",
    "        z_mask = torch.ones((bs, self.latent_size),\n",
    "                            dtype=bool,\n",
    "                            device=z.device)\n",
    "        augmask = torch.cat((z_mask, mask), axis=1)\n",
    "        # xseq = self.query_pos_decoder(xseq)\n",
    "        output = self.decoder( xseq, xseq)[z.shape[0]:]\n",
    "        # print(output.shape)\n",
    "        output = self.final_layer(output)\n",
    "        # zero for padded area\n",
    "        output[~mask.T] = 0\n",
    "        # Pytorch Transformer: [Sequence, Batch size, ...]\n",
    "        feats = output.permute(1, 0, 2)\n",
    "        return feats\n",
    "\n",
    "print(motion_seq.shape)\n",
    "motion_seq = motion_seq.reshape(motion_seq.shape[0], motion_seq.shape[1], -1)\n",
    "print(motion_seq.shape)\n",
    "\n",
    "latent_dim = 256\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "d_model = 256\n",
    "dim_feedforward = latent_dim\n",
    "\n",
    "transformer = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "out = transformer(motion_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,\tBatch 0,\tLoss 1.2867529392242432 \tTime 48.2876410484314\n",
      "Epoch 0,\tBatch 1,\tLoss 1.1176049709320068 \tTime 38.748793840408325\n",
      "Epoch 0,\tBatch 2,\tLoss 0.8456929326057434 \tTime 38.37087106704712\n",
      "Epoch 0,\tBatch 3,\tLoss 0.9594648480415344 \tTime 39.6554491519928\n",
      "Epoch 0,\tBatch 4,\tLoss 0.6949453949928284 \tTime 38.96312999725342\n",
      "Epoch 0,\tBatch 5,\tLoss 0.7214362621307373 \tTime 39.29506492614746\n",
      "Epoch 0,\tBatch 6,\tLoss 0.7119171619415283 \tTime 38.70934700965881\n",
      "Epoch 0,\tBatch 7,\tLoss 0.7455111145973206 \tTime 39.22153401374817\n",
      "Epoch 0,\tBatch 8,\tLoss 0.564123272895813 \tTime 38.63730025291443\n",
      "Epoch 0,\tBatch 9,\tLoss 0.4911697208881378 \tTime 40.80144262313843\n",
      "Epoch 0,\tBatch 10,\tLoss 0.7143735289573669 \tTime 42.58835577964783\n",
      "Epoch 0,\tBatch 11,\tLoss 0.5333280563354492 \tTime 40.67775797843933\n",
      "Epoch 0,\tBatch 12,\tLoss 0.45885419845581055 \tTime 40.33706283569336\n",
      "Epoch 0,\tBatch 13,\tLoss 0.4927862584590912 \tTime 38.58107805252075\n",
      "Epoch 0,\tBatch 14,\tLoss 0.5787313580513 \tTime 40.18187093734741\n",
      "Epoch 0,\tBatch 15,\tLoss 0.7161564826965332 \tTime 41.24776005744934\n",
      "Epoch 0,\tBatch 16,\tLoss 0.7473982572555542 \tTime 40.74867820739746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bn, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dm\u001b[38;5;241m.\u001b[39mtrain_dataloader()):\n\u001b[1;32m     25\u001b[0m     batch_start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTime \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime()\u001b[38;5;241m-\u001b[39mbatch_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(motion_seq)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, motion_seq)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/t2mENV/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from time import time\n",
    "from utils import plot_3d_motion_animation\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(transformer.parameters(), lr=1e-4)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def train_step(batch):\n",
    "    motion_seq, text_enc = batch\n",
    "    motion_seq = motion_seq.reshape(motion_seq.shape[0], motion_seq.shape[1], -1)\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(motion_seq)\n",
    "    loss = loss_fn(output, motion_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for bn, batch in enumerate(dm.train_dataloader()):\n",
    "        batch_start = time()\n",
    "        loss = train_step(batch)\n",
    "        print(f'Epoch {epoch},\\tBatch {bn},\\tLoss {loss}', f'\\tTime {time()-batch_start}')\n",
    "    print(f'Epoch {epoch} Loss {loss}')\n",
    "    output = transformer(motion_seq)\n",
    "    plot_3d_motion_animation(output[0].detach().cpu().numpy(), title='Generated Motion', save_path=f'output_{epoch}.mp4')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_widths': [256, 256, 256], 'dropout': 0.1, 'activation': 'relu', 'out_dim': 128}\n",
      "pose0: torch.Size([2, 1, 22, 3])\n",
      "velocity_relative: torch.Size([2, 159, 22, 3])\n",
      "root_travel: torch.Size([2, 160, 1, 3])\n",
      "torch.Size([2, 11040])\n",
      "torch.Size([2, 160, 128])\n",
      "torch.Size([2, 160, 128])\n",
      "torch.Size([2, 20480])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "z shape:      torch.Size([2, 256])\n",
      "z shape:      torch.Size([2, 20480])\n",
      "z shape:      torch.Size([2, 160, 128])\n",
      "z shape:      torch.Size([2, 160, 128])\n",
      "z shape:      torch.Size([2, 20480])\n",
      "torch.Size([2, 11040])\n",
      "{'mse_rel': tensor(5.1785, grad_fn=<MseLossBackward0>), 'mse_root': tensor(0.5949, grad_fn=<MseLossBackward0>), 'klDiv': tensor(1.2031, grad_fn=<MulBackward0>), 'total': tensor(5.7735, grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from utils import plot_3d_motion_frames_multiple, plot_3d_motion_animation, plot_3d_motion_frames_multiple\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "activation_dict = {\n",
    "    'tanh': nn.Tanh(),\n",
    "    'leaky_relu': nn.LeakyReLU(),\n",
    "    'relu': nn.ReLU(),\n",
    "    'sigmoid': nn.Sigmoid(),\n",
    "    'elu': nn.ELU(),\n",
    "    'swish': nn.SiLU(),\n",
    "    'mish': nn.Mish(),\n",
    "    'softplus': nn.Softplus(),\n",
    "    'softsign': nn.Softsign(),\n",
    "    # 'bent_identity': nn.BentIdentity(),\n",
    "    # 'gaussian': nn.Gaussian(),\n",
    "    'softmax': nn.Softmax(),\n",
    "    'softmin': nn.Softmin(),\n",
    "    'softshrink': nn.Softshrink(),\n",
    "    # 'sinc': nn.Sinc(),\n",
    "}\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, klDiv_weight=0.000002, relative_weight=1,  root_weight=1):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.klDiv_weight = klDiv_weight\n",
    "\n",
    "    \n",
    "    def forward(self, rel_rec, rel, root_rec, root, mu, logvar):\n",
    "        mse_rel = F.mse_loss(rel_rec, rel)\n",
    "        mse_root = F.mse_loss(root_rec, root)\n",
    "        klDiv = self.kl_divergence(mu, logvar)\n",
    "\n",
    "        loss = {\n",
    "            'mse_rel': mse_rel,\n",
    "            'mse_root': mse_root,\n",
    "            'klDiv': klDiv,\n",
    "            'total': mse_rel + mse_root + self.klDiv_weight * klDiv\n",
    "        }\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def kl_divergence(self, mu, logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "\n",
    "# no activation class\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class TransformerMotionAutoencoder_Chunked(nn.Module):\n",
    "    \"\"\"\n",
    "    We want this class to recieve:\n",
    "        pose0: (batch_size, 1, num_joints=22, 3)\n",
    "        velocity_relative: (batch_size, seq_len-1, num_joints=22, 3)\n",
    "        root_travel: (batch_size, seq_len, 1, 3)\n",
    "    from the dataloader and output:\n",
    "        the same pose0,  velocity_relative, root_travel\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "\n",
    "    ):\n",
    "        super(TransformerMotionAutoencoder_Chunked, self).__init__()\n",
    "\n",
    "        # data things\n",
    "        self.seq_len = cfg.get(\"seq_len\", 160)\n",
    "        self.input_dim = cfg.get(\"input_dim\", 66)\n",
    "        \n",
    "        # model things\n",
    "        self.hidden_dim = cfg.get(\"hidden_dim\", 1024)\n",
    "        self.n_layers = cfg.get(\"n_layers\", 8)\n",
    "        self.n_heads = cfg.get(\"n_heads\", 6)\n",
    "        self.dropout = cfg.get(\"dropout\", 0.10)\n",
    "        self.latent_dim = cfg.get(\"latent_dim\", 256)\n",
    "        self.activation = cfg.get(\"activation\", \"relu\")\n",
    "        self.activation = activation_dict[self.activation]\n",
    "        self.transformer_activation = cfg.get(\"transformer_activation\", \"gelu\")\n",
    "        self.output_layer = cfg.get(\"output_layer\", \"linear\")\n",
    "        self.clip = cfg.get(\"clip_grad_norm\", 1)\n",
    "        self.batch_norm = cfg.get(\"batch_norm\", False)\n",
    "        self.hindden_encoder_layer_widths = cfg.get(\"hidden_encoder_layer_widths\", [256] * 3 )\n",
    "\n",
    "        # training things\n",
    "        self.lr = cfg.get(\"learning_rate\", 1 * 1e-5)\n",
    "        self.optimizer = cfg.get(\"optimizer\", \"AdamW\")\n",
    "        self.load = cfg.get(\"load\", False)\n",
    "        self.checkpoint_path = cfg.get(\"_checkpoint_path\", None)\n",
    "\n",
    "        # logging things\n",
    "        self.save_animations = cfg.get(\"_save_animations\", True)\n",
    "        self.loss_function = CustomLoss(cfg.get(\"klDiv_weight\", 0.000001))\n",
    "\n",
    "\n",
    "        ##### MODEL #####\n",
    "        # the strucure will be as follows:\n",
    "        # 0. con\n",
    "        ## 1. encoder\n",
    "            ## chunked transformer\n",
    "            ## concat\n",
    "            ## linear layer\n",
    "        ## reparameterize\n",
    "        ## 2. decoder\n",
    "            ## linear layer\n",
    "            ## chunked transformer\n",
    "            ## concat\n",
    "        \n",
    "        # vel transformer encoder\n",
    "        cfg_vte = cfg.MODEL.vel_transformer_encoder\n",
    "        self.vel_transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=66,\n",
    "                nhead=cfg_vte.get(\"nhead\", 6),\n",
    "                dim_feedforward=cfg_vte.get(\"dim_feedforward\", 1024),\n",
    "                dropout=cfg_vte.get(\"dropout\", 0.1),\n",
    "                activation=cfg_vte.get(\"activation\", \"relu\"),\n",
    "            ),\n",
    "            num_layers=cfg_vte.get(\"num_layers\", 6),\n",
    "        ) \n",
    "\n",
    "        # root transformer encoder\n",
    "        cfg_rte = cfg.MODEL.root_travel_transformer_encoder\n",
    "        self.root_travel_transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=3,\n",
    "                nhead=cfg_rte.get(\"nhead\", 3),\n",
    "                dim_feedforward=cfg_rte.get(\"dim_feedforward\", 512),\n",
    "                dropout=cfg_rte.get(\"dropout\", 0.1),\n",
    "                activation=cfg_rte.get(\"activation\", \"relu\"),\n",
    "            ),\n",
    "            num_layers=cfg_rte.get(\"num_layers\", 6),\n",
    "        )\n",
    "        \n",
    "        # linear encoder\n",
    "        cfg_le = cfg.MODEL.linear_encoder\n",
    "        self.linear_encoder = nn.Sequential()\n",
    "        current_dim = (self.seq_len -1 ) * 66 + 3 * self.seq_len + 66\n",
    "        in_dims = [current_dim] + cfg_le.get(\"hidden_encoder_layer_widths\", [256] * 3)\n",
    "        out_dims = cfg_le.get(\"hidden_encoder_layer_widths\", [256] * 3) + [2 * self.latent_dim]\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.linear_encoder.add_module(f\"linear_{i}\", nn.Linear(in_dim, out_dim))\n",
    "            if i< len(in_dims) - 1:\n",
    "                self.linear_encoder.add_module(f\"activation_{i}\", activation_dict[cfg_le.get(\"activation\", \"relu\")])\n",
    "                if self.batch_norm:\n",
    "                    self.linear_encoder.add_module(f\"batch_norm_{i}\", nn.BatchNorm1d(out_dim))\n",
    "                self.linear_encoder.add_module(f\"dropout_{i}\", nn.Dropout(cfg_le.get(\"dropout\", 0.1)))\n",
    "\n",
    "\n",
    "        # linear decoder\n",
    "        cfg_ld = cfg.MODEL.linear_decoder\n",
    "        self.linear_decoder = nn.Sequential()\n",
    "        target_dim = current_dim\n",
    "        current_dim = self.latent_dim\n",
    "        in_dims = [current_dim] + cfg_ld.get(\"hidden_decoder_layer_widths\", [256] * 3)\n",
    "        out_dims = cfg_ld.get(\"hidden_decoder_layer_widths\", [256] * 3) + [target_dim]\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.linear_decoder.add_module(f\"linear_{i}\", nn.Linear(in_dim, out_dim))\n",
    "            # if i>0:\n",
    "            self.linear_decoder.add_module(f\"activation_{i}\", activation_dict[cfg_ld.get(\"activation\", \"relu\")])\n",
    "            if self.batch_norm:\n",
    "                self.linear_decoder.add_module(f\"batch_norm_{i}\", nn.BatchNorm1d(out_dim))\n",
    "            self.linear_decoder.add_module(f\"dropout_{i}\", nn.Dropout(cfg_ld.get(\"dropout\", 0.1)))\n",
    "\n",
    "        # vel transformer decoder\n",
    "        cfg_vtd = cfg.MODEL.vel_transformer_decoder\n",
    "        self.vel_transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=66,\n",
    "                nhead=cfg_vtd.get(\"nhead\", 6),\n",
    "                dim_feedforward=cfg_vtd.get(\"dim_feedforward\", 1024),\n",
    "                dropout=cfg_vtd.get(\"dropout\", 0.1),\n",
    "                activation=cfg_vtd.get(\"activation\", \"relu\"),\n",
    "            ),\n",
    "            num_layers=cfg_vtd.get(\"num_layers\", 6),\n",
    "        )\n",
    "    \n",
    "        # root transformer decoder\n",
    "        cfg_rtd = cfg.MODEL.root_travel_transformer_decoder\n",
    "        self.root_travel_transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=3,\n",
    "                nhead=cfg_rtd.get(\"nhead\", 3),\n",
    "                dim_feedforward=cfg_rtd.get(\"dim_feedforward\", 512),\n",
    "                dropout=cfg_rtd.get(\"dropout\", 0.1),\n",
    "                activation=cfg_rtd.get(\"activation\", \"relu\"),\n",
    "            ),\n",
    "            num_layers=cfg_rtd.get(\"num_layers\", 6),\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "    def encode(self, pose0,  velocity_relative, root_travel):\n",
    "        # pose0: (batch_size, 1, num_joints=22, 3)\n",
    "        # velocity_relative: (batch_size, seq_len-1, num_joints=22, 3)\n",
    "        # root_travel: (batch_size, seq_len, 1, 3)\n",
    "\n",
    "        # 1. encoder\n",
    "        # chunked transformer for vel and roof\n",
    "        # concat\n",
    "        # linear layer\n",
    "        pose0 = pose0.view(-1, 66)\n",
    "        velocity_relative = velocity_relative.view(-1, self.seq_len-1, 66)\n",
    "        root_travel = root_travel.view(-1, self.seq_len, 3)\n",
    "\n",
    "        # vel\n",
    "        vel = self.vel_transformer_encoder(velocity_relative,)\n",
    "        # root\n",
    "        root = self.root_travel_transformer_encoder(root_travel, )\n",
    "\n",
    "        print('vel:', vel.shape)\n",
    "        print('root:', root.shape)\n",
    "        print('pose0:', pose0.shape)\n",
    "        # concat\n",
    "        x = torch.cat([nn.Flatten()(vel), nn.Flatten()(root), pose0], dim=1)\n",
    "        x = self.linear_encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu.__add__(eps.__mul__(std))\n",
    "    \n",
    "    def decode(self, z):\n",
    "        # linear layer\n",
    "        # chunked transformer\n",
    "        # concat\n",
    "        z = self.linear_decoder(z)\n",
    "        # vel\n",
    "        print(z.shape)\n",
    "        pose0 = z[:, -66:].view(-1, 1, 22, 3)\n",
    "        vel = z[:, :(self.seq_len-1) * 66].view(-1, self.seq_len-1, 22* 3)\n",
    "        root = z[:, (self.seq_len-1) * 66:-66].view(-1, self.seq_len, 3)\n",
    "\n",
    "        vel = self.vel_transformer_decoder(vel, vel)\n",
    "        root = self.root_travel_transformer_decoder(root, root)\n",
    "\n",
    "        return pose0, vel, root\n",
    "    \n",
    "    def forward(self, pose0,  velocity_relative, root_travel):\n",
    "        mu, logvar = self.encode(pose0,  velocity_relative, root_travel)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        pose0_rec, vel_rec, root_rec = self.decode(z)\n",
    "        \n",
    "        return pose0_rec, vel_rec, root_rec, mu, logvar\n",
    "    \n",
    "    \n",
    "class TransformerMotionAutoencoder_Concatenated(nn.Module):\n",
    "    \"\"\"\n",
    "    We want this class to recieve:\n",
    "        pose0: (batch_size, 1, num_joints=22, 3)\n",
    "        velocity_relative: (batch_size, seq_len-1, num_joints=22, 3)\n",
    "        root_travel: (batch_size, seq_len, 1, 3)\n",
    "    from the dataloader and output:\n",
    "        the same pose0,  velocity_relative, root_travel\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "\n",
    "    ):\n",
    "        super(TransformerMotionAutoencoder_Concatenated, self).__init__()\n",
    "\n",
    "        # data things\n",
    "        self.seq_len = cfg.get(\"seq_len\", 160)\n",
    "        self.input_dim = cfg.get(\"input_dim\", 66)\n",
    "        \n",
    "        # model things\n",
    "        self.hidden_dim = cfg.get(\"hidden_dim\", 1024)\n",
    "        self.n_layers = cfg.get(\"n_layers\", 8)\n",
    "        self.n_heads = cfg.get(\"n_heads\", 6)\n",
    "        self.dropout = cfg.get(\"dropout\", 0.10)\n",
    "        self.latent_dim = cfg.get(\"latent_dim\", 256)\n",
    "        self.activation = cfg.get(\"activation\", \"relu\")\n",
    "        self.activation = activation_dict[self.activation]\n",
    "        self.transformer_activation = cfg.get(\"transformer_activation\", \"gelu\")\n",
    "        self.output_layer = cfg.get(\"output_layer\", \"linear\")\n",
    "        self.clip = cfg.get(\"clip_grad_norm\", 1)\n",
    "        self.batch_norm = cfg.get(\"batch_norm\", False)\n",
    "        self.hindden_encoder_layer_widths = cfg.get(\"hidden_encoder_layer_widths\", [256] * 3 )\n",
    "\n",
    "        # training things\n",
    "        self.lr = cfg.get(\"learning_rate\", 1 * 1e-5)\n",
    "        self.optimizer = cfg.get(\"optimizer\", \"AdamW\")\n",
    "        self.load = cfg.get(\"load\", False)\n",
    "        self.checkpoint_path = cfg.get(\"_checkpoint_path\", None)\n",
    "\n",
    "        # logging things\n",
    "        self.save_animations = cfg.get(\"_save_animations\", True)\n",
    "        self.loss_function = CustomLoss(cfg.get(\"klDiv_weight\", 0.000001))\n",
    "\n",
    "\n",
    "        ##### MODEL #####\n",
    "        # the strucure will be as follows:\n",
    "        # 0. concat\n",
    "        ## 1. encoder\n",
    "            ## linear\n",
    "            ##  transformer\n",
    "            ## flatten\n",
    "            ## linear layer\n",
    "        ## reparameterize\n",
    "        ## 2. decoder\n",
    "            ## linear layer\n",
    "            ## chunked transformer\n",
    "            ## concat\n",
    "\n",
    "        cfg_lei = cfg.MODEL.CONCAT_TRANSFORMER.linear_encoder_input\n",
    "        print(cfg_lei)\n",
    "        self.linear_encoder_input = nn.Sequential()\n",
    "        current_dim = (self.seq_len -1 ) * 66 + 3 * self.seq_len + 66\n",
    "        in_dims = [current_dim] + cfg_lei.get(\"hidden_encoder_layer_widths\", [256] * 3)\n",
    "        out_dims = cfg_lei.get(\"hidden_encoder_layer_widths\", [256] * 3) + [self.seq_len*cfg_lei.get(\"out_dim\", 128)]\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.linear_encoder_input.add_module(f\"linear_{i}\", nn.Linear(in_dim, out_dim))\n",
    "            self.linear_encoder_input.add_module(f\"activation_{i}\", activation_dict[cfg_lei.get(\"activation\", \"relu\")])\n",
    "            if self.batch_norm:\n",
    "                self.linear_encoder_input.add_module(f\"batch_norm_{i}\", nn.BatchNorm1d(out_dim))\n",
    "            self.linear_encoder_input.add_module(f\"dropout_{i}\", nn.Dropout(cfg_lei.get(\"dropout\", 0.1)))\n",
    "\n",
    "\n",
    "        # transformer encoder\n",
    "        cfg_te = cfg.MODEL.CONCAT_TRANSFORMER.transformer_encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=cfg_lei.get(\"out_dim\", 128),\n",
    "                nhead=cfg_te.get(\"nhead\", 8),\n",
    "                dim_feedforward=cfg_te.get(\"dim_feedforward\", 1024),\n",
    "                dropout=cfg_te.get(\"dropout\", 0.1),\n",
    "                activation=cfg_te.get(\"activation\", \"relu\"),\n",
    "            ),\n",
    "            num_layers=cfg_te.get(\"num_layers\", 6),\n",
    "        ) \n",
    "        # linear encoder\n",
    "        cfg_le = cfg.MODEL.CONCAT_TRANSFORMER.linear_encoder_output\n",
    "        self.linear_encoder = nn.Sequential()\n",
    "        current_dim = self.seq_len * cfg_lei.get(\"out_dim\", 128)\n",
    "        in_dims = [current_dim] + cfg_le.get(\"hidden_encoder_layer_widths\", [256] * 3)\n",
    "        out_dims = cfg_le.get(\"hidden_encoder_layer_widths\", [256] * 3) + [2 * self.latent_dim]\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.linear_encoder.add_module(f\"linear_{i}\", nn.Linear(in_dim, out_dim))\n",
    "            if i< len(in_dims) - 1:\n",
    "                self.linear_encoder.add_module(f\"activation_{i}\", activation_dict[cfg_le.get(\"activation\", \"relu\")])\n",
    "                if self.batch_norm:\n",
    "                    self.linear_encoder.add_module(f\"batch_norm_{i}\", nn.BatchNorm1d(out_dim))\n",
    "                self.linear_encoder.add_module(f\"dropout_{i}\", nn.Dropout(cfg_le.get(\"dropout\", 0.1)))\n",
    "\n",
    "\n",
    "        # linear decoder\n",
    "        cfg_ldi = cfg.MODEL.CONCAT_TRANSFORMER.linear_decoder_input\n",
    "        self.linear_decoder_input = nn.Sequential()\n",
    "        target_dim = current_dim\n",
    "        current_dim = self.latent_dim\n",
    "        in_dims = [current_dim] + cfg_ldi.get(\"hidden_decoder_layer_widths\", [256] * 3)\n",
    "        out_dims = cfg_ldi.get(\"hidden_decoder_layer_widths\", [256] * 3) + [target_dim]\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.linear_decoder_input.add_module(f\"linear_{i}\", nn.Linear(in_dim, out_dim))\n",
    "            # if i>0:\n",
    "            self.linear_decoder_input.add_module(f\"activation_{i}\", activation_dict[cfg_ldi.get(\"activation\", \"relu\")])\n",
    "            if self.batch_norm:\n",
    "                self.linear_decoder_input.add_module(f\"batch_norm_{i}\", nn.BatchNorm1d(out_dim))\n",
    "            self.linear_decoder_input.add_module(f\"dropout_{i}\", nn.Dropout(cfg_ldi.get(\"dropout\", 0.1)))\n",
    "\n",
    "        # transformer decoder\n",
    "        cfg_td = cfg.MODEL.CONCAT_TRANSFORMER.transformer_decoder\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=cfg_lei.get(\"out_dim\", 128),\n",
    "                nhead=cfg_td.get(\"nhead\", 8),\n",
    "                dim_feedforward=cfg_td.get(\"dim_feedforward\", 1024),\n",
    "                dropout=cfg_td.get(\"dropout\", 0.1),\n",
    "                activation=cfg_td.get(\"activation\", \"relu\"),\n",
    "            ),\n",
    "            num_layers=cfg_td.get(\"num_layers\", 6),\n",
    "        )\n",
    "\n",
    "        # linear output\n",
    "        cfg_ldo = cfg.MODEL.CONCAT_TRANSFORMER.linear_decoder_output\n",
    "        self.linear_decoder_output = nn.Sequential()\n",
    "        target_dim = (self.seq_len -1 ) * 66 + 3 * self.seq_len + 66\n",
    "        current_dim = cfg_ldi.get(\"out_dim\", 128) * self.seq_len\n",
    "        in_dims = [current_dim] + cfg_ldo.get(\"hidden_decoder_layer_widths\", [256] * 3)\n",
    "        out_dims = cfg_ldo.get(\"hidden_decoder_layer_widths\", [256] * 3) + [target_dim]\n",
    "\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.linear_decoder_output.add_module(f\"linear_{i}\", nn.Linear(in_dim, out_dim))\n",
    "            # if i>0:\n",
    "            self.linear_decoder_output.add_module(f\"activation_{i}\", activation_dict[cfg_ldo.get(\"activation\", \"relu\")])\n",
    "            if self.batch_norm:\n",
    "                self.linear_decoder_output.add_module(f\"batch_norm_{i}\", nn.BatchNorm1d(out_dim))\n",
    "            self.linear_decoder_output.add_module(f\"dropout_{i}\", nn.Dropout(cfg_ldo.get(\"dropout\", 0.1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, pose0,  velocity_relative, root_travel):\n",
    "        # pose0: (batch_size, 1, num_joints=22, 3)\n",
    "        # velocity_relative: (batch_size, seq_len-1, num_joints=22, 3)\n",
    "        # root_travel: (batch_size, seq_len, 1, 3)\n",
    "\n",
    "        # 1. encoder\n",
    "        # concat\n",
    "        # linear layer\n",
    "        # transformer encoder\n",
    "\n",
    "        x = torch.cat([nn.Flatten()(velocity_relative), nn.Flatten()(root_travel), nn.Flatten()(pose0)], dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.linear_encoder_input(x)\n",
    "        x = x.view(-1, self.seq_len, cfg.MODEL.CONCAT_TRANSFORMER.linear_encoder_input.get(\"out_dim\", 128))\n",
    "        print(x.shape)\n",
    "        x = self.transformer_encoder(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, self.seq_len * cfg.MODEL.CONCAT_TRANSFORMER.linear_encoder_input.get(\"out_dim\", 128))\n",
    "        print(x.shape)\n",
    "        x = self.linear_encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=1)\n",
    "        print(mu.shape, logvar.shape)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu.__add__(eps.__mul__(std))\n",
    "    \n",
    "    def decode(self, z):\n",
    "        out_dim =  cfg.MODEL.CONCAT_TRANSFORMER.linear_encoder_input.get(\"out_dim\", 128)\n",
    "        print('z shape:     ', z.shape)\n",
    "\n",
    "        z = self.linear_decoder_input(z)\n",
    "        print('z shape:     ', z.shape)\n",
    "        z = z.view(-1, self.seq_len, out_dim)\n",
    "        print('z shape:     ', z.shape)\n",
    "\n",
    "        z = self.transformer_decoder(z, z)\n",
    "        print('z shape:     ', z.shape)\n",
    "        z = z.view(-1, self.seq_len * out_dim)\n",
    "        print('z shape:     ', z.shape)\n",
    "        z = self.linear_decoder_output(z)\n",
    "        # vel\n",
    "        print(z.shape)\n",
    "        pose0 = z[:, -66:].view(-1, 1, 22, 3)\n",
    "        vel = z[:, :(self.seq_len-1) * 66].view(-1, self.seq_len-1, 22, 3)\n",
    "        root = z[:, (self.seq_len-1) * 66:-66].view(-1, self.seq_len, 1,  3)\n",
    "\n",
    "\n",
    "        return pose0, vel, root\n",
    "    \n",
    "    def forward(self, pose0,  velocity_relative, root_travel):\n",
    "        mu, logvar = self.encode(pose0,  velocity_relative, root_travel)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        pose0_rec, vel_rec, root_rec = self.decode(z)\n",
    "        relative_motion = self.reconstruct(pose0_rec, vel_rec)\n",
    "        return relative_motion, root_rec, mu, logvar\n",
    "\n",
    "    def reconstruct(self, pose0,  velocity_relative):\n",
    "        motion_less_root = torch.cumsum(torch.cat([pose0, velocity_relative], dim=1), dim=1)\n",
    "        return motion_less_root\n",
    "            \n",
    "\n",
    "# try it out on batch\n",
    "def reconstruct(pose0,  velocity_relative):\n",
    "    motion_less_root = torch.cumsum(torch.cat([pose0, velocity_relative], dim=1), dim=1)\n",
    "    return motion_less_root\n",
    "from config import config as cfg # to force reload: del cfg; from config import config as cfg\n",
    "model = TransformerMotionAutoencoder_Concatenated(cfg)  \n",
    "\n",
    "pose0,  velocity_relative, root_travel, motion_less_root, motion_seq = batch\n",
    "rel = reconstruct(pose0,  velocity_relative)\n",
    "print('pose0:', pose0.shape)\n",
    "print('velocity_relative:', velocity_relative.shape)\n",
    "print('root_travel:', root_travel.shape)\n",
    "\n",
    "rel_rec, root_rec, mu, logvar = model(pose0,  velocity_relative, root_travel)\n",
    "# get loss\n",
    "loss = model.loss_function(rel_rec, rel, root_rec, root_travel, mu, logvar)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer_widths': [256, 256, 256], 'dropout': 0.1, 'activation': 'relu'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.MODEL.CONCAT_TRANSFORMER.linear_decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint_path = '../tb_logs5/TransformerMotionAutoencoder/version_85/checkpoints/epoch=34-step=4515.ckpt'\n",
    "weights = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "decoder_linear_block.0.weight torch.Size([1024, 1024])\n",
      "decoder_linear_block.0.bias torch.Size([1024])\n",
      "decoder_linear_block.3.weight torch.Size([4096, 1024])\n",
      "decoder_linear_block.3.bias torch.Size([4096])\n",
      "decoder_linear_block.6.weight torch.Size([8192, 4096])\n",
      "decoder_linear_block.6.bias torch.Size([8192])\n",
      "decoder_linear_block.9.weight torch.Size([10560, 8192])\n",
      "decoder_linear_block.9.bias torch.Size([10560])\n",
      "transformer_decoder.layers.0.self_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.0.self_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.0.self_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.0.self_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.0.multihead_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.0.multihead_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.0.multihead_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.0.multihead_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.0.linear1.weight torch.Size([1024, 66])\n",
      "transformer_decoder.layers.0.linear1.bias torch.Size([1024])\n",
      "transformer_decoder.layers.0.linear2.weight torch.Size([66, 1024])\n",
      "transformer_decoder.layers.0.linear2.bias torch.Size([66])\n",
      "transformer_decoder.layers.0.norm1.weight torch.Size([66])\n",
      "transformer_decoder.layers.0.norm1.bias torch.Size([66])\n",
      "transformer_decoder.layers.0.norm2.weight torch.Size([66])\n",
      "transformer_decoder.layers.0.norm2.bias torch.Size([66])\n",
      "transformer_decoder.layers.0.norm3.weight torch.Size([66])\n",
      "transformer_decoder.layers.0.norm3.bias torch.Size([66])\n",
      "transformer_decoder.layers.1.self_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.1.self_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.1.self_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.1.self_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.1.multihead_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.1.multihead_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.1.multihead_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.1.multihead_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.1.linear1.weight torch.Size([1024, 66])\n",
      "transformer_decoder.layers.1.linear1.bias torch.Size([1024])\n",
      "transformer_decoder.layers.1.linear2.weight torch.Size([66, 1024])\n",
      "transformer_decoder.layers.1.linear2.bias torch.Size([66])\n",
      "transformer_decoder.layers.1.norm1.weight torch.Size([66])\n",
      "transformer_decoder.layers.1.norm1.bias torch.Size([66])\n",
      "transformer_decoder.layers.1.norm2.weight torch.Size([66])\n",
      "transformer_decoder.layers.1.norm2.bias torch.Size([66])\n",
      "transformer_decoder.layers.1.norm3.weight torch.Size([66])\n",
      "transformer_decoder.layers.1.norm3.bias torch.Size([66])\n",
      "transformer_decoder.layers.2.self_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.2.self_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.2.self_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.2.self_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.2.multihead_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.2.multihead_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.2.multihead_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.2.multihead_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.2.linear1.weight torch.Size([1024, 66])\n",
      "transformer_decoder.layers.2.linear1.bias torch.Size([1024])\n",
      "transformer_decoder.layers.2.linear2.weight torch.Size([66, 1024])\n",
      "transformer_decoder.layers.2.linear2.bias torch.Size([66])\n",
      "transformer_decoder.layers.2.norm1.weight torch.Size([66])\n",
      "transformer_decoder.layers.2.norm1.bias torch.Size([66])\n",
      "transformer_decoder.layers.2.norm2.weight torch.Size([66])\n",
      "transformer_decoder.layers.2.norm2.bias torch.Size([66])\n",
      "transformer_decoder.layers.2.norm3.weight torch.Size([66])\n",
      "transformer_decoder.layers.2.norm3.bias torch.Size([66])\n",
      "transformer_decoder.layers.3.self_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.3.self_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.3.self_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.3.self_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.3.multihead_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.3.multihead_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.3.multihead_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.3.multihead_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.3.linear1.weight torch.Size([1024, 66])\n",
      "transformer_decoder.layers.3.linear1.bias torch.Size([1024])\n",
      "transformer_decoder.layers.3.linear2.weight torch.Size([66, 1024])\n",
      "transformer_decoder.layers.3.linear2.bias torch.Size([66])\n",
      "transformer_decoder.layers.3.norm1.weight torch.Size([66])\n",
      "transformer_decoder.layers.3.norm1.bias torch.Size([66])\n",
      "transformer_decoder.layers.3.norm2.weight torch.Size([66])\n",
      "transformer_decoder.layers.3.norm2.bias torch.Size([66])\n",
      "transformer_decoder.layers.3.norm3.weight torch.Size([66])\n",
      "transformer_decoder.layers.3.norm3.bias torch.Size([66])\n",
      "transformer_decoder.layers.4.self_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.4.self_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.4.self_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.4.self_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.4.multihead_attn.in_proj_weight torch.Size([198, 66])\n",
      "transformer_decoder.layers.4.multihead_attn.in_proj_bias torch.Size([198])\n",
      "transformer_decoder.layers.4.multihead_attn.out_proj.weight torch.Size([66, 66])\n",
      "transformer_decoder.layers.4.multihead_attn.out_proj.bias torch.Size([66])\n",
      "transformer_decoder.layers.4.linear1.weight torch.Size([1024, 66])\n",
      "transformer_decoder.layers.4.linear1.bias torch.Size([1024])\n",
      "transformer_decoder.layers.4.linear2.weight torch.Size([66, 1024])\n",
      "transformer_decoder.layers.4.linear2.bias torch.Size([66])\n",
      "transformer_decoder.layers.4.norm1.weight torch.Size([66])\n",
      "transformer_decoder.layers.4.norm1.bias torch.Size([66])\n",
      "transformer_decoder.layers.4.norm2.weight torch.Size([66])\n",
      "transformer_decoder.layers.4.norm2.bias torch.Size([66])\n",
      "transformer_decoder.layers.4.norm3.weight torch.Size([66])\n",
      "transformer_decoder.layers.4.norm3.bias torch.Size([66])\n",
      "output_block.1.weight torch.Size([10560, 10560])\n",
      "output_block.1.bias torch.Size([10560])\n"
     ]
    }
   ],
   "source": [
    "print(len(weights['state_dict'].keys()))\n",
    "enc_weight_keys = list(weights['state_dict'].keys())[:70]\n",
    "dec_weight_keys = list(weights['state_dict'].keys())[70:]\n",
    "for key in dec_weight_keys:\n",
    "    print(key, weights['state_dict'][key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "def lengths_to_mask(lengths: List[int],\n",
    "                    device: torch.device,\n",
    "                    max_len: int = None) -> Tensor:\n",
    "    \"\"\"\n",
    "    Provides a mask, of length max_len or the longest element in lengths. With True for the elements less than the length for each length in lengths.\n",
    "    \"\"\"\n",
    "    lengths = torch.tensor(lengths, device=device)\n",
    "    max_len = max_len if max_len else max(lengths)\n",
    "    mask = torch.arange(max_len, device=device).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "lengths = [10, 20, 30]\n",
    "\n",
    "mask = lengths_to_mask(lengths, torch.device('cpu'))\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2mENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
